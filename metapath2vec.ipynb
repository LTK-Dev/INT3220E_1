{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd81c984",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-20T16:56:10.354021Z",
     "iopub.status.busy": "2025-11-20T16:56:10.353823Z",
     "iopub.status.idle": "2025-11-20T16:56:12.232308Z",
     "shell.execute_reply": "2025-11-20T16:56:12.231480Z"
    },
    "papermill": {
     "duration": 1.883692,
     "end_time": "2025-11-20T16:56:12.233658",
     "exception": false,
     "start_time": "2025-11-20T16:56:10.349966",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/graph-ben-ngoai/__results__.html\n",
      "/kaggle/input/graph-ben-ngoai/__notebook__.ipynb\n",
      "/kaggle/input/graph-ben-ngoai/__output__.json\n",
      "/kaggle/input/graph-ben-ngoai/custom.css\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/train_val_malicious_path_UCC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/normal_path_CUC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/test_malicious_path_UCCA.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/all_malicious_path_UCAC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/normal_path_UCC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/graph_data_torch.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/test_malicious_path_UCC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/test_malicious_path_UCAC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/normal_path_UCAC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/train_val_malicious_path_UCCA.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/all_malicious_path_UCCA.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/train_val_malicious_path_CUC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/test_malicious_path_CUC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/computer2nodeid.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/process2nodeid.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/user2nodeid.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/graph_data_dgl.bin\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/all_malicious_path_CUC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/all_malicious_path_UCC.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/normal_path_UCCA.pt\n",
      "/kaggle/input/graph-ben-ngoai/graph_data_20251120161219/train_val_malicious_path_UCAC.pt\n",
      "/kaggle/input/lmtracker/__results__.html\n",
      "/kaggle/input/lmtracker/__notebook__.ipynb\n",
      "/kaggle/input/lmtracker/__output__.json\n",
      "/kaggle/input/lmtracker/custom.css\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/malicious_test_path.pt\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/graph_data_torch.pt\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/malicious_train_val_path.pt\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/computer2nodeid.pt\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/process2nodeid.pt\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/user2nodeid.pt\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/normal_path.pt\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/graph_data_dgl.bin\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904/malicious_CUC_path.pt\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945dc645",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T16:56:12.239668Z",
     "iopub.status.busy": "2025-11-20T16:56:12.239385Z",
     "iopub.status.idle": "2025-11-20T17:00:17.250006Z",
     "shell.execute_reply": "2025-11-20T17:00:17.249157Z"
    },
    "papermill": {
     "duration": 245.01516,
     "end_time": "2025-11-20T17:00:17.251492",
     "exception": false,
     "start_time": "2025-11-20T16:56:12.236332",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping torch-scatter as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-sparse as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-cluster as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-spline-conv as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch-geometric as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Skipping pyg-lib as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m799.1/799.1 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m88.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m76.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m176.2/176.2 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.4/209.4 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m10.9/10.9 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m89.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m989.8/989.8 kB\u001b[0m \u001b[31m39.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/1.4 GB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "nx-cugraph-cu12 25.6.0 requires pylibcugraph-cu12==25.6.*, but you have pylibcugraph-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torch-scatter torch-sparse torch-cluster torch-spline-conv torch-geometric pyg-lib -q\n",
    "\n",
    "!pip install torch==2.4.0+cu121 torchvision==0.19.0+cu121 torchaudio==2.4.0+cu121 --extra-index-url https://download.pytorch.org/whl/cu121 -q\n",
    "\n",
    "!pip install torch_scatter torch_sparse torch_cluster torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html -q\n",
    "!pip install torch_geometric -q\n",
    "\n",
    "!pip install pyg_lib torch_spline_conv -f https://data.pyg.org/whl/torch-2.4.0+cu121.html -q\n",
    "\n",
    "!pip install libcugraph-cu12==25.2.* pylibcugraph-cu12==25.2.* libraft-cu12==25.2.* rmm-cu12==25.2.* --extra-index-url=https://pypi.nvidia.com -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e5c706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:00:17.332988Z",
     "iopub.status.busy": "2025-11-20T17:00:17.332415Z",
     "iopub.status.idle": "2025-11-20T17:00:30.984821Z",
     "shell.execute_reply": "2025-11-20T17:00:30.984092Z"
    },
    "papermill": {
     "duration": 13.693758,
     "end_time": "2025-11-20T17:00:30.986127",
     "exception": false,
     "start_time": "2025-11-20T17:00:17.292369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… torch_scatter OK\n",
      "âœ… torch_sparse OK\n",
      "âœ… torch_cluster OK\n",
      "âœ… torch_spline_conv OK\n",
      "PyG: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import torch_scatter, torch_sparse, torch_cluster, torch_spline_conv\n",
    "import torch_geometric\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "print(\"âœ… torch_scatter OK\")\n",
    "print(\"âœ… torch_sparse OK\")\n",
    "print(\"âœ… torch_cluster OK\")\n",
    "print(\"âœ… torch_spline_conv OK\")\n",
    "print(\"PyG:\", torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb997ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:00:31.065639Z",
     "iopub.status.busy": "2025-11-20T17:00:31.065102Z",
     "iopub.status.idle": "2025-11-20T17:00:31.071812Z",
     "shell.execute_reply": "2025-11-20T17:00:31.071216Z"
    },
    "papermill": {
     "duration": 0.047271,
     "end_time": "2025-11-20T17:00:31.072874",
     "exception": false,
     "start_time": "2025-11-20T17:00:31.025603",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ca78e5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:00:31.151170Z",
     "iopub.status.busy": "2025-11-20T17:00:31.150680Z",
     "iopub.status.idle": "2025-11-20T17:00:31.169992Z",
     "shell.execute_reply": "2025-11-20T17:00:31.169361Z"
    },
    "papermill": {
     "duration": 0.059729,
     "end_time": "2025-11-20T17:00:31.171089",
     "exception": false,
     "start_time": "2025-11-20T17:00:31.111360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Searching for lmTracker output folder...\n",
      "âœ… Found lmTracker directory:\n",
      "/kaggle/input/lmtracker/graph_data_20251119174904\n",
      "\n",
      "ðŸ“‚ Files inside this directory:\n",
      "   â”œâ”€ computer2nodeid.pt\n",
      "   â”œâ”€ graph_data_dgl.bin\n",
      "   â”œâ”€ graph_data_torch.pt\n",
      "   â”œâ”€ malicious_CUC_path.pt\n",
      "   â”œâ”€ malicious_test_path.pt\n",
      "   â”œâ”€ malicious_train_val_path.pt\n",
      "   â”œâ”€ normal_path.pt\n",
      "   â”œâ”€ process2nodeid.pt\n",
      "   â”œâ”€ user2nodeid.pt\n",
      "\n",
      "\n",
      "ðŸ“Œ Checking required files...\n",
      "\n",
      "âœ… Found: graph_data_torch.pt\n",
      "âœ… Found: normal_path.pt\n",
      "âœ… Found: malicious_CUC_path.pt\n",
      "âœ… Found: malicious_train_val_path.pt\n",
      "âœ… Found: malicious_test_path.pt\n",
      "âœ… Found: computer2nodeid.pt\n",
      "âœ… Found: user2nodeid.pt\n",
      "\n",
      "ðŸŽ‰ All required files exist! Dataset is READY.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "REQUIRED_FILES = [\n",
    "    \"graph_data_torch.pt\",\n",
    "    \"normal_path.pt\",\n",
    "    \"malicious_CUC_path.pt\",\n",
    "    \"malicious_train_val_path.pt\",\n",
    "    \"malicious_test_path.pt\",\n",
    "    \"computer2nodeid.pt\",\n",
    "    \"user2nodeid.pt\",\n",
    "]\n",
    "\n",
    "def find_lmtracker_dir(base_dir=\"/kaggle/input/lmtracker\"):\n",
    "    \n",
    "    if \"graph_data_torch.pt\" in os.listdir(base_dir):\n",
    "        return base_dir\n",
    "\n",
    "    for root, dirs, files in os.walk(base_dir):\n",
    "        if \"graph_data_torch.pt\" in files:\n",
    "            return root\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "print(\"ðŸ” Searching for lmTracker output folder...\")\n",
    "\n",
    "lm_dir = find_lmtracker_dir()\n",
    "\n",
    "if lm_dir is None:\n",
    "    raise FileNotFoundError(\n",
    "        \"âŒ KhÃ´ng tÃ¬m tháº¥y thÆ° má»¥c chá»©a graph_data_torch.pt trong /kaggle/input/lmtracker.\\n\"\n",
    "        \"âž¡ HÃ£y kiá»ƒm tra láº¡i báº¡n Ä‘Ã£ attach Ä‘Ãºng dataset lmTracker hay chÆ°a.\"\n",
    "    )\n",
    "\n",
    "print(f\"âœ… Found lmTracker directory:\\n{lm_dir}\\n\")\n",
    "\n",
    "# In toÃ n bá»™ file cÃ³ trong thÆ° má»¥c Ä‘Ã³\n",
    "print(\"ðŸ“‚ Files inside this directory:\")\n",
    "for f in sorted(os.listdir(lm_dir)):\n",
    "    print(\"   â”œâ”€\", f)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Check thiáº¿u file nÃ o khÃ´ng\n",
    "print(\"ðŸ“Œ Checking required files...\\n\")\n",
    "missing_files = []\n",
    "\n",
    "for fname in REQUIRED_FILES:\n",
    "    fpath = os.path.join(lm_dir, fname)\n",
    "    if not os.path.isfile(fpath):\n",
    "        missing_files.append(fname)\n",
    "        print(f\"âŒ Missing: {fname}\")\n",
    "    else:\n",
    "        print(f\"âœ… Found: {fname}\")\n",
    "\n",
    "if missing_files:\n",
    "    print(\"\\nâš ï¸ WARNING: Dataset thiáº¿u file sau:\")\n",
    "    for f in missing_files:\n",
    "        print(\"   -\", f)\n",
    "    print(\"\\nâ— Code MetaPath2Vec sáº½ lá»—i náº¿u thiáº¿u cÃ¡c file nÃ y.\")\n",
    "else:\n",
    "    print(\"\\nðŸŽ‰ All required files exist! Dataset is READY.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a808f4",
   "metadata": {
    "papermill": {
     "duration": 0.037405,
     "end_time": "2025-11-20T17:00:31.247161",
     "exception": false,
     "start_time": "2025-11-20T17:00:31.209756",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c06250b5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:00:31.324504Z",
     "iopub.status.busy": "2025-11-20T17:00:31.324289Z",
     "iopub.status.idle": "2025-11-20T17:00:31.348555Z",
     "shell.execute_reply": "2025-11-20T17:00:31.347862Z"
    },
    "papermill": {
     "duration": 0.06421,
     "end_time": "2025-11-20T17:00:31.349627",
     "exception": false,
     "start_time": "2025-11-20T17:00:31.285417",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from distutils.log import WARN\n",
    "from pdb import post_mortem\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.nn import Embedding\n",
    "from torch.utils.data import DataLoader\n",
    "from torch_sparse import SparseTensor\n",
    "\n",
    "from torch_geometric.typing import EdgeType, NodeType, OptTensor\n",
    "\n",
    "EPS = 1e-15\n",
    "\n",
    "\n",
    "class MetaPath2Vec(torch.nn.Module):\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        edge_index_dict: Dict[EdgeType, Tensor],\n",
    "        embedding_dim: int,\n",
    "        metapath: List[EdgeType],\n",
    "        walk_length: int,\n",
    "        context_size: int,\n",
    "        walks_per_node: int = 1,\n",
    "        num_negative_samples: int = 1,\n",
    "        num_nodes_dict: Optional[Dict[NodeType, int]] = None,\n",
    "        sparse: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if num_nodes_dict is None:\n",
    "            num_nodes_dict = {}\n",
    "            for keys, edge_index in edge_index_dict.items():\n",
    "                key = keys[0]\n",
    "                N = int(edge_index[0].max() + 1)\n",
    "                num_nodes_dict[key] = max(N, num_nodes_dict.get(key, N))\n",
    "\n",
    "                key = keys[-1]\n",
    "                N = int(edge_index[1].max() + 1)\n",
    "                num_nodes_dict[key] = max(N, num_nodes_dict.get(key, N))\n",
    "        adj_dict = {}\n",
    "        for keys, edge_index in edge_index_dict.items():\n",
    "            sizes = (num_nodes_dict[keys[0]], num_nodes_dict[keys[-1]])\n",
    "            row, col = edge_index\n",
    "            # print(edge_index)\n",
    "            adj = SparseTensor(row=row, col=col, sparse_sizes=sizes)\n",
    "            adj = adj.to('cpu')\n",
    "            adj_dict[keys] = adj\n",
    "        assert walk_length + 1 >= context_size\n",
    "        if walk_length > len(metapath) and metapath[0][0] != metapath[-1][-1]:\n",
    "            raise AttributeError(\n",
    "                \"The 'walk_length' is longer than the given 'metapath', but \"\n",
    "                \"the 'metapath' does not denote a cycle\")\n",
    "\n",
    "        self.adj_dict = adj_dict\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.metapath = metapath\n",
    "        self.walk_length = walk_length\n",
    "        self.context_size = context_size\n",
    "        self.walks_per_node = walks_per_node\n",
    "        self.num_negative_samples = num_negative_samples\n",
    "        self.num_nodes_dict = num_nodes_dict\n",
    "\n",
    "        types = set([x[0] for x in metapath]) | set([x[-1] for x in metapath])\n",
    "        types = sorted(list(types))\n",
    "        count = 0\n",
    "        self.start, self.end = {}, {}\n",
    "        for key in types:\n",
    "            self.start[key] = count\n",
    "            count += num_nodes_dict[key]\n",
    "            self.end[key] = count\n",
    "        offset = [self.start[metapath[0][0]]]\n",
    "        offset += [self.start[keys[-1]] for keys in metapath\n",
    "                   ]* int((walk_length / len(metapath)) + 1)\n",
    "        offset = offset[:walk_length + 1]\n",
    "        assert len(offset) == walk_length + 1\n",
    "        self.offset = torch.tensor(offset)\n",
    "\n",
    "        # + 1 denotes a dummy node used to link to for isolated nodes.\n",
    "        self.embedding = Embedding(count + 1, embedding_dim, sparse=sparse)\n",
    "        self.dummy_idx = count\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.embedding.reset_parameters()\n",
    "\n",
    "    def forward(self, node_type: str, batch: OptTensor = None) -> Tensor:\n",
    "        r\"\"\"Returns the embeddings for the nodes in :obj:`batch` of type\n",
    "        :obj:`node_type`.\"\"\"\n",
    "        emb = self.embedding.weight[self.start[node_type]:self.end[node_type]]\n",
    "        return emb if batch is None else emb.index_select(0, batch)\n",
    "\n",
    "    def loader(self, **kwargs):\n",
    "        r\"\"\"Returns the data loader that creates both positive and negative\n",
    "        random walks on the heterogeneous graph.\n",
    "\n",
    "        Args:\n",
    "            **kwargs (optional): Arguments of\n",
    "                :class:`torch.utils.data.DataLoader`, such as\n",
    "                :obj:`batch_size`, :obj:`shuffle`, :obj:`drop_last` or\n",
    "                :obj:`num_workers`.\n",
    "        \"\"\"\n",
    "        return DataLoader(range(self.num_nodes_dict[self.metapath[0][0]]),\n",
    "                          collate_fn=self._sample, **kwargs)\n",
    "\n",
    "    def _pos_sample(self, batch: Tensor) -> Tensor:\n",
    "\n",
    "        batch = batch.repeat(self.walks_per_node)\n",
    "\n",
    "        rws = [batch]\n",
    "        for i in range(self.walk_length):\n",
    "            keys = self.metapath[i % len(self.metapath)]\n",
    "            adj = self.adj_dict[keys]\n",
    "            batch = sample(adj, batch, num_neighbors=1,\n",
    "                           dummy_idx=self.dummy_idx).view(-1)\n",
    "            rws.append(batch)\n",
    "\n",
    "        rw = torch.stack(rws, dim=-1)\n",
    "        rw.add_(self.offset.view(1, -1))\n",
    "        rw[rw > self.dummy_idx] = self.dummy_idx\n",
    "\n",
    "\n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "\n",
    "        return torch.cat(walks, dim=0)\n",
    "\n",
    "    def _neg_sample(self, batch: Tensor) -> Tensor:\n",
    "        batch = batch.repeat(self.walks_per_node * self.num_negative_samples)\n",
    "\n",
    "        rws = [batch]\n",
    "        for i in range(self.walk_length):\n",
    "            keys = self.metapath[i % len(self.metapath)]\n",
    "            batch = torch.randint(0, self.num_nodes_dict[keys[-1]],\n",
    "                                  (batch.size(0), ), dtype=torch.long)\n",
    "            rws.append(batch)\n",
    "\n",
    "        rw = torch.stack(rws, dim=-1)\n",
    "        rw.add_(self.offset.view(1, -1))\n",
    "\n",
    "        walks = []\n",
    "        num_walks_per_rw = 1 + self.walk_length + 1 - self.context_size\n",
    "        for j in range(num_walks_per_rw):\n",
    "            walks.append(rw[:, j:j + self.context_size])\n",
    "        return torch.cat(walks, dim=0)\n",
    "\n",
    "    def _sample(self, batch: List[int]) -> Tuple[Tensor, Tensor]:\n",
    "        if not isinstance(batch, Tensor):\n",
    "            batch = torch.tensor(batch, dtype=torch.long)\n",
    "        return self._pos_sample(batch), self._neg_sample(batch)\n",
    "\n",
    "    def loss(self, pos_rw: Tensor, neg_rw: Tensor) -> Tensor:\n",
    "        r\"\"\"Computes the loss given positive and negative random walks.\"\"\"\n",
    "        start, rest = pos_rw[:, 0], pos_rw[:, 1:].contiguous()\n",
    "    \n",
    "        h_start = self.embedding(start).view(pos_rw.size(0), 1,\n",
    "                                             self.embedding_dim)\n",
    "        h_rest = self.embedding(rest.view(-1)).view(pos_rw.size(0), -1,\n",
    "                                                    self.embedding_dim)\n",
    "\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        pos_loss = -torch.log(torch.sigmoid(out) + EPS).mean()\n",
    "\n",
    "        # Negative loss.\n",
    "        start, rest = neg_rw[:, 0], neg_rw[:, 1:].contiguous()\n",
    "\n",
    "        h_start = self.embedding(start).view(neg_rw.size(0), 1,\n",
    "                                             self.embedding_dim)\n",
    "        h_rest = self.embedding(rest.view(-1)).view(neg_rw.size(0), -1,\n",
    "                                                    self.embedding_dim)\n",
    "        out = (h_start * h_rest).sum(dim=-1).view(-1)\n",
    "        neg_loss = -torch.log(1 - torch.sigmoid(out) + EPS).mean()\n",
    "\n",
    "        return pos_loss + neg_loss\n",
    "\n",
    "    def get_embedding(self, pos_rw: Tensor) -> Tensor:\n",
    "        r\"\"\"Computes the loss given positive and negative random walks.\"\"\"\n",
    "\n",
    "        # Positive loss.\n",
    "\n",
    "        start, rest = pos_rw[0], pos_rw[1:].contiguous()\n",
    "\n",
    "        # h_start = self.embedding(start).view(pos_rw.size(0), 1,\n",
    "        #                                      self.embedding_dim)\n",
    "        # h_rest = self.embedding(rest.view(-1)).view(pos_rw.size(0), -1,\n",
    "        #                                             self.embedding_dim)\n",
    "        h_start = self.embedding(start)\n",
    "        h_rest = self.embedding(rest.view(-1))\n",
    "        # print(h_start.size())\n",
    "        # print(h_rest.size())\n",
    "        out = (h_start * h_rest).mean(dim=0)\n",
    "        # out = (h_start * h_rest)\n",
    "        return out\n",
    "\n",
    "\n",
    "        \n",
    "    def test(self, train_z: Tensor, train_y: Tensor, test_z: Tensor,\n",
    "             test_y: Tensor, solver: str = \"lbfgs\", multi_class: str = \"auto\",\n",
    "             *args, **kwargs) -> float:\n",
    "        r\"\"\"Evaluates latent space quality via a logistic regression downstream\n",
    "        task.\"\"\"\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "        clf = LogisticRegression(solver=solver, multi_class=multi_class, *args,\n",
    "                                 **kwargs).fit(train_z.detach().cpu().numpy(),\n",
    "                                               train_y.detach().cpu().numpy())\n",
    "        return clf.score(test_z.detach().cpu().numpy(),\n",
    "                         test_y.detach().cpu().numpy())\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return (f'{self.__class__.__name__}('\n",
    "                f'{self.embedding.weight.size(0) - 1}, '\n",
    "                f'{self.embedding.weight.size(1)})')\n",
    "\n",
    "\n",
    "def sample(src: SparseTensor, subset: Tensor, num_neighbors: int,\n",
    "           dummy_idx: int) -> Tensor:\n",
    "\n",
    "    mask = subset < dummy_idx\n",
    "    rowcount = torch.zeros_like(subset)\n",
    "    rowcount[mask] = src.storage.rowcount()[subset[mask]]\n",
    "    mask = mask & (rowcount > 0)\n",
    "    offset = torch.zeros_like(subset)\n",
    "    offset[mask] = src.storage.rowptr()[subset[mask]]\n",
    "\n",
    "    rand = torch.rand((rowcount.size(0), num_neighbors), device=subset.device)\n",
    "    rand.mul_(rowcount.to(rand.dtype).view(-1, 1))\n",
    "    rand = rand.to(torch.long)\n",
    "    rand.add_(offset.view(-1, 1))\n",
    "\n",
    "    col = src.storage.col()[rand]\n",
    "    col[~mask] = dummy_idx\n",
    "    return col"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b398af",
   "metadata": {
    "papermill": {
     "duration": 0.037309,
     "end_time": "2025-11-20T17:00:31.424247",
     "exception": false,
     "start_time": "2025-11-20T17:00:31.386938",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "MetaPath2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c20c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T17:00:31.500663Z",
     "iopub.status.busy": "2025-11-20T17:00:31.500435Z",
     "iopub.status.idle": "2025-11-20T17:56:43.183345Z",
     "shell.execute_reply": "2025-11-20T17:56:43.182645Z"
    },
    "papermill": {
     "duration": 3371.73135,
     "end_time": "2025-11-20T17:56:43.192981",
     "exception": false,
     "start_time": "2025-11-20T17:00:31.461631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/2816664050.py:22: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  data = torch.load(dir_save + '/graph_data_torch.pt')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HeteroData(\n",
      "  (Computer, Connect, Computer)={ edge_index=[2, 429628] },\n",
      "  (User, Logon, Computer)={ edge_index=[2, 272027] },\n",
      "  (Computer, Logon_rev, User)={ edge_index=[2, 272027] },\n",
      "  (User, SwitchUser, User)={ edge_index=[2, 5634] },\n",
      "  (Computer, Use, Auth_Type)={ edge_index=[2, 44531] },\n",
      "  (Auth_Type, Use_rev, Computer)={ edge_index=[2, 44531] },\n",
      "  (Computer, Use_logon, Logon_type)={ edge_index=[2, 64316] },\n",
      "  (Logon_type, Use_logon_rev, Computer)={ edge_index=[2, 64316] },\n",
      "  (Computer, Have, Logon_orient)={ edge_index=[2, 61530] },\n",
      "  (Logon_orient, Have_rev, Computer)={ edge_index=[2, 61530] }\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 6 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Computer': 0, 'User': 15425}\n",
      "{'Computer': 15425, 'User': 27013}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15425/15425 [00:33<00:00, 454.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([    1, 19233,    70, 22324,  8552], device='cuda:0')\n",
      "30850\n",
      "Epoch: 1, Step: 00100/483, Loss: 7.8509\n",
      "Epoch: 1, Step: 00200/483, Loss: 4.9164\n",
      "Epoch: 1, Step: 00300/483, Loss: 3.4112\n",
      "Epoch: 1, Step: 00400/483, Loss: 2.5252\n",
      "2.5251856446266174\n",
      "--- 32.501702308654785 seconds ---\n",
      "Epoch: 2, Step: 00100/483, Loss: 1.6327\n",
      "Epoch: 2, Step: 00200/483, Loss: 1.3773\n",
      "Epoch: 2, Step: 00300/483, Loss: 1.2190\n",
      "Epoch: 2, Step: 00400/483, Loss: 1.1232\n",
      "1.1232260751724243\n",
      "--- 64.68127775192261 seconds ---\n",
      "Epoch: 3, Step: 00100/483, Loss: 1.0313\n",
      "Epoch: 3, Step: 00200/483, Loss: 1.0047\n",
      "Epoch: 3, Step: 00300/483, Loss: 0.9864\n",
      "Epoch: 3, Step: 00400/483, Loss: 0.9726\n",
      "0.972641304731369\n",
      "--- 96.91701054573059 seconds ---\n",
      "Epoch: 4, Step: 00100/483, Loss: 0.9565\n",
      "Epoch: 4, Step: 00200/483, Loss: 0.9508\n",
      "Epoch: 4, Step: 00300/483, Loss: 0.9453\n",
      "Epoch: 4, Step: 00400/483, Loss: 0.9409\n",
      "0.9409071958065033\n",
      "--- 129.41843843460083 seconds ---\n",
      "Epoch: 5, Step: 00100/483, Loss: 0.9358\n",
      "Epoch: 5, Step: 00200/483, Loss: 0.9324\n",
      "Epoch: 5, Step: 00300/483, Loss: 0.9300\n",
      "Epoch: 5, Step: 00400/483, Loss: 0.9282\n",
      "0.9281600296497345\n",
      "--- 161.80828714370728 seconds ---\n",
      "Epoch: 6, Step: 00100/483, Loss: 0.9261\n",
      "Epoch: 6, Step: 00200/483, Loss: 0.9240\n",
      "Epoch: 6, Step: 00300/483, Loss: 0.9226\n",
      "Epoch: 6, Step: 00400/483, Loss: 0.9216\n",
      "0.9216335135698318\n",
      "--- 194.26087141036987 seconds ---\n",
      "Epoch: 7, Step: 00100/483, Loss: 0.9209\n",
      "Epoch: 7, Step: 00200/483, Loss: 0.9193\n",
      "Epoch: 7, Step: 00300/483, Loss: 0.9182\n",
      "Epoch: 7, Step: 00400/483, Loss: 0.9175\n",
      "0.9174619889259339\n",
      "--- 226.7026834487915 seconds ---\n",
      "Epoch: 8, Step: 00100/483, Loss: 0.9176\n",
      "Epoch: 8, Step: 00200/483, Loss: 0.9158\n",
      "Epoch: 8, Step: 00300/483, Loss: 0.9154\n",
      "Epoch: 8, Step: 00400/483, Loss: 0.9147\n",
      "0.9146594101190567\n",
      "--- 259.1025457382202 seconds ---\n",
      "Epoch: 9, Step: 00100/483, Loss: 0.9149\n",
      "Epoch: 9, Step: 00200/483, Loss: 0.9139\n",
      "Epoch: 9, Step: 00300/483, Loss: 0.9138\n",
      "Epoch: 9, Step: 00400/483, Loss: 0.9130\n",
      "0.91303571164608\n",
      "--- 291.6160538196564 seconds ---\n",
      "Epoch: 10, Step: 00100/483, Loss: 0.9135\n",
      "Epoch: 10, Step: 00200/483, Loss: 0.9121\n",
      "Epoch: 10, Step: 00300/483, Loss: 0.9118\n",
      "Epoch: 10, Step: 00400/483, Loss: 0.9114\n",
      "0.9114163959026337\n",
      "--- 324.1547701358795 seconds ---\n",
      "Epoch: 11, Step: 00100/483, Loss: 0.9122\n",
      "Epoch: 11, Step: 00200/483, Loss: 0.9107\n",
      "Epoch: 11, Step: 00300/483, Loss: 0.9101\n",
      "Epoch: 11, Step: 00400/483, Loss: 0.9097\n",
      "0.9097193747758865\n",
      "--- 356.5623035430908 seconds ---\n",
      "Epoch: 12, Step: 00100/483, Loss: 0.9107\n",
      "Epoch: 12, Step: 00200/483, Loss: 0.9095\n",
      "Epoch: 12, Step: 00300/483, Loss: 0.9089\n",
      "Epoch: 12, Step: 00400/483, Loss: 0.9088\n",
      "0.9087695515155793\n",
      "--- 388.9722650051117 seconds ---\n",
      "Epoch: 13, Step: 00100/483, Loss: 0.9096\n",
      "Epoch: 13, Step: 00200/483, Loss: 0.9085\n",
      "Epoch: 13, Step: 00300/483, Loss: 0.9073\n",
      "Epoch: 13, Step: 00400/483, Loss: 0.9074\n",
      "0.9074342679977417\n",
      "--- 421.4856321811676 seconds ---\n",
      "Epoch: 14, Step: 00100/483, Loss: 0.9079\n",
      "Epoch: 14, Step: 00200/483, Loss: 0.9069\n",
      "Epoch: 14, Step: 00300/483, Loss: 0.9058\n",
      "Epoch: 14, Step: 00400/483, Loss: 0.9059\n",
      "0.905940163731575\n",
      "--- 453.9446074962616 seconds ---\n",
      "Epoch: 15, Step: 00100/483, Loss: 0.9068\n",
      "Epoch: 15, Step: 00200/483, Loss: 0.9055\n",
      "Epoch: 15, Step: 00300/483, Loss: 0.9049\n",
      "Epoch: 15, Step: 00400/483, Loss: 0.9046\n",
      "0.9045550942420959\n",
      "--- 486.41874742507935 seconds ---\n",
      "Epoch: 16, Step: 00100/483, Loss: 0.9055\n",
      "Epoch: 16, Step: 00200/483, Loss: 0.9046\n",
      "Epoch: 16, Step: 00300/483, Loss: 0.9041\n",
      "Epoch: 16, Step: 00400/483, Loss: 0.9037\n",
      "0.9037412494421005\n",
      "--- 518.8594162464142 seconds ---\n",
      "Epoch: 17, Step: 00100/483, Loss: 0.9045\n",
      "Epoch: 17, Step: 00200/483, Loss: 0.9033\n",
      "Epoch: 17, Step: 00300/483, Loss: 0.9028\n",
      "Epoch: 17, Step: 00400/483, Loss: 0.9026\n",
      "0.9026327282190323\n",
      "--- 551.4089403152466 seconds ---\n",
      "Epoch: 18, Step: 00100/483, Loss: 0.9033\n",
      "Epoch: 18, Step: 00200/483, Loss: 0.9025\n",
      "Epoch: 18, Step: 00300/483, Loss: 0.9018\n",
      "Epoch: 18, Step: 00400/483, Loss: 0.9018\n",
      "0.9017778593301773\n",
      "--- 583.9206535816193 seconds ---\n",
      "Epoch: 19, Step: 00100/483, Loss: 0.9029\n",
      "Epoch: 19, Step: 00200/483, Loss: 0.9010\n",
      "Epoch: 19, Step: 00300/483, Loss: 0.9011\n",
      "Epoch: 19, Step: 00400/483, Loss: 0.9008\n",
      "0.9008353269100189\n",
      "--- 616.3314402103424 seconds ---\n",
      "Epoch: 20, Step: 00100/483, Loss: 0.9012\n",
      "Epoch: 20, Step: 00200/483, Loss: 0.9004\n",
      "Epoch: 20, Step: 00300/483, Loss: 0.8998\n",
      "Epoch: 20, Step: 00400/483, Loss: 0.8997\n",
      "0.8996670591831207\n",
      "--- 648.7478921413422 seconds ---\n",
      "Epoch: 21, Step: 00100/483, Loss: 0.9006\n",
      "Epoch: 21, Step: 00200/483, Loss: 0.8995\n",
      "Epoch: 21, Step: 00300/483, Loss: 0.8993\n",
      "Epoch: 21, Step: 00400/483, Loss: 0.8991\n",
      "0.8991042816638947\n",
      "--- 681.201868057251 seconds ---\n",
      "Epoch: 22, Step: 00100/483, Loss: 0.9002\n",
      "Epoch: 22, Step: 00200/483, Loss: 0.8988\n",
      "Epoch: 22, Step: 00300/483, Loss: 0.8986\n",
      "Epoch: 22, Step: 00400/483, Loss: 0.8982\n",
      "0.8981862193346024\n",
      "--- 713.6739943027496 seconds ---\n",
      "Epoch: 23, Step: 00100/483, Loss: 0.8991\n",
      "Epoch: 23, Step: 00200/483, Loss: 0.8985\n",
      "Epoch: 23, Step: 00300/483, Loss: 0.8982\n",
      "Epoch: 23, Step: 00400/483, Loss: 0.8982\n",
      "0.8982183122634888\n",
      "--- 746.0487322807312 seconds ---\n",
      "Epoch: 24, Step: 00100/483, Loss: 0.8984\n",
      "Epoch: 24, Step: 00200/483, Loss: 0.8977\n",
      "Epoch: 24, Step: 00300/483, Loss: 0.8975\n",
      "Epoch: 24, Step: 00400/483, Loss: 0.8975\n",
      "0.8974742496013641\n",
      "--- 778.6358964443207 seconds ---\n",
      "Epoch: 25, Step: 00100/483, Loss: 0.8980\n",
      "Epoch: 25, Step: 00200/483, Loss: 0.8973\n",
      "Epoch: 25, Step: 00300/483, Loss: 0.8972\n",
      "Epoch: 25, Step: 00400/483, Loss: 0.8970\n",
      "0.8970493614673615\n",
      "--- 811.1674194335938 seconds ---\n",
      "Epoch: 26, Step: 00100/483, Loss: 0.8977\n",
      "Epoch: 26, Step: 00200/483, Loss: 0.8970\n",
      "Epoch: 26, Step: 00300/483, Loss: 0.8972\n",
      "Epoch: 26, Step: 00400/483, Loss: 0.8966\n",
      "0.8966209721565247\n",
      "--- 843.6337552070618 seconds ---\n",
      "Epoch: 27, Step: 00100/483, Loss: 0.8975\n",
      "Epoch: 27, Step: 00200/483, Loss: 0.8966\n",
      "Epoch: 27, Step: 00300/483, Loss: 0.8967\n",
      "Epoch: 27, Step: 00400/483, Loss: 0.8961\n",
      "0.8960968077182769\n",
      "--- 875.9935214519501 seconds ---\n",
      "Epoch: 28, Step: 00100/483, Loss: 0.8970\n",
      "Epoch: 28, Step: 00200/483, Loss: 0.8965\n",
      "Epoch: 28, Step: 00300/483, Loss: 0.8966\n",
      "Epoch: 28, Step: 00400/483, Loss: 0.8966\n",
      "0.896551970243454\n",
      "--- 908.5482106208801 seconds ---\n",
      "Epoch: 29, Step: 00100/483, Loss: 0.8968\n",
      "Epoch: 29, Step: 00200/483, Loss: 0.8962\n",
      "Epoch: 29, Step: 00300/483, Loss: 0.8960\n",
      "Epoch: 29, Step: 00400/483, Loss: 0.8964\n",
      "0.8963872218132019\n",
      "--- 941.0267975330353 seconds ---\n",
      "Epoch: 30, Step: 00100/483, Loss: 0.8967\n",
      "Epoch: 30, Step: 00200/483, Loss: 0.8960\n",
      "Epoch: 30, Step: 00300/483, Loss: 0.8960\n",
      "Epoch: 30, Step: 00400/483, Loss: 0.8957\n",
      "0.895749284029007\n",
      "--- 973.5047056674957 seconds ---\n",
      "Epoch: 31, Step: 00100/483, Loss: 0.8962\n",
      "Epoch: 31, Step: 00200/483, Loss: 0.8954\n",
      "Epoch: 31, Step: 00300/483, Loss: 0.8957\n",
      "Epoch: 31, Step: 00400/483, Loss: 0.8954\n",
      "0.895354768037796\n",
      "--- 1005.8790605068207 seconds ---\n",
      "Epoch: 32, Step: 00100/483, Loss: 0.8962\n",
      "Epoch: 32, Step: 00200/483, Loss: 0.8961\n",
      "Epoch: 32, Step: 00300/483, Loss: 0.8955\n",
      "Epoch: 32, Step: 00400/483, Loss: 0.8954\n",
      "0.8954411637783051\n",
      "--- 1038.347950220108 seconds ---\n",
      "Epoch: 33, Step: 00100/483, Loss: 0.8960\n",
      "Epoch: 33, Step: 00200/483, Loss: 0.8955\n",
      "Epoch: 33, Step: 00300/483, Loss: 0.8954\n",
      "Epoch: 33, Step: 00400/483, Loss: 0.8953\n",
      "0.8952990245819091\n",
      "--- 1070.7748439311981 seconds ---\n",
      "Epoch: 34, Step: 00100/483, Loss: 0.8957\n",
      "Epoch: 34, Step: 00200/483, Loss: 0.8956\n",
      "Epoch: 34, Step: 00300/483, Loss: 0.8952\n",
      "Epoch: 34, Step: 00400/483, Loss: 0.8951\n",
      "0.8951362037658691\n",
      "--- 1103.3104531764984 seconds ---\n",
      "Epoch: 35, Step: 00100/483, Loss: 0.8960\n",
      "Epoch: 35, Step: 00200/483, Loss: 0.8952\n",
      "Epoch: 35, Step: 00300/483, Loss: 0.8951\n",
      "Epoch: 35, Step: 00400/483, Loss: 0.8953\n",
      "0.8952546012401581\n",
      "--- 1135.803759098053 seconds ---\n",
      "Epoch: 36, Step: 00100/483, Loss: 0.8960\n",
      "Epoch: 36, Step: 00200/483, Loss: 0.8952\n",
      "Epoch: 36, Step: 00300/483, Loss: 0.8952\n",
      "Epoch: 36, Step: 00400/483, Loss: 0.8951\n",
      "0.8951341980695724\n",
      "--- 1168.2395701408386 seconds ---\n",
      "Epoch: 37, Step: 00100/483, Loss: 0.8964\n",
      "Epoch: 37, Step: 00200/483, Loss: 0.8950\n",
      "Epoch: 37, Step: 00300/483, Loss: 0.8948\n",
      "Epoch: 37, Step: 00400/483, Loss: 0.8950\n",
      "0.8949942392110825\n",
      "--- 1200.726318359375 seconds ---\n",
      "Epoch: 38, Step: 00100/483, Loss: 0.8953\n",
      "Epoch: 38, Step: 00200/483, Loss: 0.8950\n",
      "Epoch: 38, Step: 00300/483, Loss: 0.8951\n",
      "Epoch: 38, Step: 00400/483, Loss: 0.8951\n",
      "0.8950947415828705\n",
      "--- 1233.1990942955017 seconds ---\n",
      "Epoch: 39, Step: 00100/483, Loss: 0.8954\n",
      "Epoch: 39, Step: 00200/483, Loss: 0.8945\n",
      "Epoch: 39, Step: 00300/483, Loss: 0.8950\n",
      "Epoch: 39, Step: 00400/483, Loss: 0.8946\n",
      "0.8946210032701493\n",
      "--- 1265.6087412834167 seconds ---\n",
      "Epoch: 40, Step: 00100/483, Loss: 0.8956\n",
      "Epoch: 40, Step: 00200/483, Loss: 0.8948\n",
      "Epoch: 40, Step: 00300/483, Loss: 0.8946\n",
      "Epoch: 40, Step: 00400/483, Loss: 0.8948\n",
      "0.8947696924209595\n",
      "--- 1298.0701041221619 seconds ---\n",
      "Epoch: 41, Step: 00100/483, Loss: 0.8951\n",
      "Epoch: 41, Step: 00200/483, Loss: 0.8949\n",
      "Epoch: 41, Step: 00300/483, Loss: 0.8946\n",
      "Epoch: 41, Step: 00400/483, Loss: 0.8948\n",
      "0.8948494148254394\n",
      "--- 1330.6835799217224 seconds ---\n",
      "Epoch: 42, Step: 00100/483, Loss: 0.8951\n",
      "Epoch: 42, Step: 00200/483, Loss: 0.8950\n",
      "Epoch: 42, Step: 00300/483, Loss: 0.8947\n",
      "Epoch: 42, Step: 00400/483, Loss: 0.8949\n",
      "0.894861860871315\n",
      "--- 1363.1139056682587 seconds ---\n",
      "Epoch: 43, Step: 00100/483, Loss: 0.8948\n",
      "Epoch: 43, Step: 00200/483, Loss: 0.8948\n",
      "Epoch: 43, Step: 00300/483, Loss: 0.8947\n",
      "Epoch: 43, Step: 00400/483, Loss: 0.8948\n",
      "0.8947668981552124\n",
      "--- 1395.5792973041534 seconds ---\n",
      "Epoch: 44, Step: 00100/483, Loss: 0.8951\n",
      "Epoch: 44, Step: 00200/483, Loss: 0.8946\n",
      "Epoch: 44, Step: 00300/483, Loss: 0.8942\n",
      "Epoch: 44, Step: 00400/483, Loss: 0.8941\n",
      "0.8941043740510941\n",
      "--- 1428.089149236679 seconds ---\n",
      "Epoch: 45, Step: 00100/483, Loss: 0.8948\n",
      "Epoch: 45, Step: 00200/483, Loss: 0.8943\n",
      "Epoch: 45, Step: 00300/483, Loss: 0.8944\n",
      "Epoch: 45, Step: 00400/483, Loss: 0.8941\n",
      "0.8940548300743103\n",
      "--- 1460.6705343723297 seconds ---\n",
      "Epoch: 46, Step: 00100/483, Loss: 0.8950\n",
      "Epoch: 46, Step: 00200/483, Loss: 0.8941\n",
      "Epoch: 46, Step: 00300/483, Loss: 0.8941\n",
      "Epoch: 46, Step: 00400/483, Loss: 0.8941\n",
      "0.8940621817111969\n",
      "--- 1493.0930597782135 seconds ---\n",
      "Epoch: 47, Step: 00100/483, Loss: 0.8947\n",
      "Epoch: 47, Step: 00200/483, Loss: 0.8942\n",
      "Epoch: 47, Step: 00300/483, Loss: 0.8940\n",
      "Epoch: 47, Step: 00400/483, Loss: 0.8943\n",
      "0.894250191450119\n",
      "--- 1525.5525069236755 seconds ---\n",
      "Epoch: 48, Step: 00100/483, Loss: 0.8946\n",
      "Epoch: 48, Step: 00200/483, Loss: 0.8942\n",
      "Epoch: 48, Step: 00300/483, Loss: 0.8940\n",
      "Epoch: 48, Step: 00400/483, Loss: 0.8942\n",
      "0.8941934305429459\n",
      "--- 1558.0530540943146 seconds ---\n",
      "Epoch: 49, Step: 00100/483, Loss: 0.8943\n",
      "Epoch: 49, Step: 00200/483, Loss: 0.8938\n",
      "Epoch: 49, Step: 00300/483, Loss: 0.8940\n",
      "Epoch: 49, Step: 00400/483, Loss: 0.8939\n",
      "0.8939141869544983\n",
      "--- 1590.5360505580902 seconds ---\n",
      "Epoch: 50, Step: 00100/483, Loss: 0.8944\n",
      "Epoch: 50, Step: 00200/483, Loss: 0.8943\n",
      "Epoch: 50, Step: 00300/483, Loss: 0.8942\n",
      "Epoch: 50, Step: 00400/483, Loss: 0.8939\n",
      "0.8939375257492066\n",
      "--- 1623.02525639534 seconds ---\n",
      "Epoch: 51, Step: 00100/483, Loss: 0.8943\n",
      "Epoch: 51, Step: 00200/483, Loss: 0.8936\n",
      "Epoch: 51, Step: 00300/483, Loss: 0.8938\n",
      "Epoch: 51, Step: 00400/483, Loss: 0.8939\n",
      "0.8939011824131012\n",
      "--- 1655.4797837734222 seconds ---\n",
      "Epoch: 52, Step: 00100/483, Loss: 0.8942\n",
      "Epoch: 52, Step: 00200/483, Loss: 0.8935\n",
      "Epoch: 52, Step: 00300/483, Loss: 0.8937\n",
      "Epoch: 52, Step: 00400/483, Loss: 0.8938\n",
      "0.8937765127420425\n",
      "--- 1687.9287769794464 seconds ---\n",
      "Epoch: 53, Step: 00100/483, Loss: 0.8939\n",
      "Epoch: 53, Step: 00200/483, Loss: 0.8938\n",
      "Epoch: 53, Step: 00300/483, Loss: 0.8940\n",
      "Epoch: 53, Step: 00400/483, Loss: 0.8938\n",
      "0.893805165886879\n",
      "--- 1720.4244656562805 seconds ---\n",
      "Epoch: 54, Step: 00100/483, Loss: 0.8933\n",
      "Epoch: 54, Step: 00200/483, Loss: 0.8937\n",
      "Epoch: 54, Step: 00300/483, Loss: 0.8933\n",
      "Epoch: 54, Step: 00400/483, Loss: 0.8935\n",
      "0.8934959131479263\n",
      "--- 1752.9081523418427 seconds ---\n",
      "Epoch: 55, Step: 00100/483, Loss: 0.8936\n",
      "Epoch: 55, Step: 00200/483, Loss: 0.8933\n",
      "Epoch: 55, Step: 00300/483, Loss: 0.8935\n",
      "Epoch: 55, Step: 00400/483, Loss: 0.8934\n",
      "0.893375289440155\n",
      "--- 1785.2444291114807 seconds ---\n",
      "Epoch: 56, Step: 00100/483, Loss: 0.8940\n",
      "Epoch: 56, Step: 00200/483, Loss: 0.8935\n",
      "Epoch: 56, Step: 00300/483, Loss: 0.8936\n",
      "Epoch: 56, Step: 00400/483, Loss: 0.8935\n",
      "0.89354294359684\n",
      "--- 1817.6310329437256 seconds ---\n",
      "Epoch: 57, Step: 00100/483, Loss: 0.8937\n",
      "Epoch: 57, Step: 00200/483, Loss: 0.8937\n",
      "Epoch: 57, Step: 00300/483, Loss: 0.8935\n",
      "Epoch: 57, Step: 00400/483, Loss: 0.8930\n",
      "0.8929750442504882\n",
      "--- 1850.0652701854706 seconds ---\n",
      "Epoch: 58, Step: 00100/483, Loss: 0.8935\n",
      "Epoch: 58, Step: 00200/483, Loss: 0.8935\n",
      "Epoch: 58, Step: 00300/483, Loss: 0.8932\n",
      "Epoch: 58, Step: 00400/483, Loss: 0.8931\n",
      "0.8931031203269959\n",
      "--- 1882.5014295578003 seconds ---\n",
      "Epoch: 59, Step: 00100/483, Loss: 0.8940\n",
      "Epoch: 59, Step: 00200/483, Loss: 0.8931\n",
      "Epoch: 59, Step: 00300/483, Loss: 0.8930\n",
      "Epoch: 59, Step: 00400/483, Loss: 0.8933\n",
      "0.8932836407423019\n",
      "--- 1914.9552743434906 seconds ---\n",
      "Epoch: 60, Step: 00100/483, Loss: 0.8934\n",
      "Epoch: 60, Step: 00200/483, Loss: 0.8929\n",
      "Epoch: 60, Step: 00300/483, Loss: 0.8932\n",
      "Epoch: 60, Step: 00400/483, Loss: 0.8931\n",
      "0.8930873060226441\n",
      "--- 1947.3376717567444 seconds ---\n",
      "Epoch: 61, Step: 00100/483, Loss: 0.8931\n",
      "Epoch: 61, Step: 00200/483, Loss: 0.8931\n",
      "Epoch: 61, Step: 00300/483, Loss: 0.8930\n",
      "Epoch: 61, Step: 00400/483, Loss: 0.8931\n",
      "0.893104366660118\n",
      "--- 1979.7367794513702 seconds ---\n",
      "Epoch: 62, Step: 00100/483, Loss: 0.8936\n",
      "Epoch: 62, Step: 00200/483, Loss: 0.8931\n",
      "Epoch: 62, Step: 00300/483, Loss: 0.8935\n",
      "Epoch: 62, Step: 00400/483, Loss: 0.8931\n",
      "0.8930574840307236\n",
      "--- 2012.1944646835327 seconds ---\n",
      "Epoch: 63, Step: 00100/483, Loss: 0.8934\n",
      "Epoch: 63, Step: 00200/483, Loss: 0.8933\n",
      "Epoch: 63, Step: 00300/483, Loss: 0.8928\n",
      "Epoch: 63, Step: 00400/483, Loss: 0.8930\n",
      "0.8930239123106003\n",
      "--- 2044.7223393917084 seconds ---\n",
      "Epoch: 64, Step: 00100/483, Loss: 0.8934\n",
      "Epoch: 64, Step: 00200/483, Loss: 0.8930\n",
      "Epoch: 64, Step: 00300/483, Loss: 0.8932\n",
      "Epoch: 64, Step: 00400/483, Loss: 0.8930\n",
      "0.8929936963319779\n",
      "--- 2077.1848378181458 seconds ---\n",
      "Epoch: 65, Step: 00100/483, Loss: 0.8929\n",
      "Epoch: 65, Step: 00200/483, Loss: 0.8930\n",
      "Epoch: 65, Step: 00300/483, Loss: 0.8928\n",
      "Epoch: 65, Step: 00400/483, Loss: 0.8925\n",
      "0.8925351274013519\n",
      "--- 2109.5308628082275 seconds ---\n",
      "Epoch: 66, Step: 00100/483, Loss: 0.8933\n",
      "Epoch: 66, Step: 00200/483, Loss: 0.8931\n",
      "Epoch: 66, Step: 00300/483, Loss: 0.8932\n",
      "Epoch: 66, Step: 00400/483, Loss: 0.8925\n",
      "0.8925383687019348\n",
      "--- 2141.9134905338287 seconds ---\n",
      "Epoch: 67, Step: 00100/483, Loss: 0.8937\n",
      "Epoch: 67, Step: 00200/483, Loss: 0.8932\n",
      "Epoch: 67, Step: 00300/483, Loss: 0.8929\n",
      "Epoch: 67, Step: 00400/483, Loss: 0.8930\n",
      "0.8929985457658768\n",
      "--- 2174.402445793152 seconds ---\n",
      "Epoch: 68, Step: 00100/483, Loss: 0.8928\n",
      "Epoch: 68, Step: 00200/483, Loss: 0.8931\n",
      "Epoch: 68, Step: 00300/483, Loss: 0.8925\n",
      "Epoch: 68, Step: 00400/483, Loss: 0.8928\n",
      "0.8928448688983918\n",
      "--- 2206.7720415592194 seconds ---\n",
      "Epoch: 69, Step: 00100/483, Loss: 0.8931\n",
      "Epoch: 69, Step: 00200/483, Loss: 0.8926\n",
      "Epoch: 69, Step: 00300/483, Loss: 0.8929\n",
      "Epoch: 69, Step: 00400/483, Loss: 0.8927\n",
      "0.892669039964676\n",
      "--- 2239.2676515579224 seconds ---\n",
      "Epoch: 70, Step: 00100/483, Loss: 0.8931\n",
      "Epoch: 70, Step: 00200/483, Loss: 0.8928\n",
      "Epoch: 70, Step: 00300/483, Loss: 0.8928\n",
      "Epoch: 70, Step: 00400/483, Loss: 0.8927\n",
      "0.8926613467931748\n",
      "--- 2271.8261983394623 seconds ---\n",
      "Epoch: 71, Step: 00100/483, Loss: 0.8933\n",
      "Epoch: 71, Step: 00200/483, Loss: 0.8926\n",
      "Epoch: 71, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 71, Step: 00400/483, Loss: 0.8928\n",
      "0.8927862423658371\n",
      "--- 2304.3017632961273 seconds ---\n",
      "Epoch: 72, Step: 00100/483, Loss: 0.8931\n",
      "Epoch: 72, Step: 00200/483, Loss: 0.8926\n",
      "Epoch: 72, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 72, Step: 00400/483, Loss: 0.8931\n",
      "0.8930898422002792\n",
      "--- 2336.825620651245 seconds ---\n",
      "Epoch: 73, Step: 00100/483, Loss: 0.8932\n",
      "Epoch: 73, Step: 00200/483, Loss: 0.8924\n",
      "Epoch: 73, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 73, Step: 00400/483, Loss: 0.8924\n",
      "0.8924328017234803\n",
      "--- 2369.324253320694 seconds ---\n",
      "Epoch: 74, Step: 00100/483, Loss: 0.8927\n",
      "Epoch: 74, Step: 00200/483, Loss: 0.8932\n",
      "Epoch: 74, Step: 00300/483, Loss: 0.8926\n",
      "Epoch: 74, Step: 00400/483, Loss: 0.8926\n",
      "0.89264344394207\n",
      "--- 2401.8604605197906 seconds ---\n",
      "Epoch: 75, Step: 00100/483, Loss: 0.8926\n",
      "Epoch: 75, Step: 00200/483, Loss: 0.8931\n",
      "Epoch: 75, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 75, Step: 00400/483, Loss: 0.8926\n",
      "0.8925939029455185\n",
      "--- 2434.344115972519 seconds ---\n",
      "Epoch: 76, Step: 00100/483, Loss: 0.8929\n",
      "Epoch: 76, Step: 00200/483, Loss: 0.8928\n",
      "Epoch: 76, Step: 00300/483, Loss: 0.8926\n",
      "Epoch: 76, Step: 00400/483, Loss: 0.8922\n",
      "0.8922419792413712\n",
      "--- 2466.745500087738 seconds ---\n",
      "Epoch: 77, Step: 00100/483, Loss: 0.8927\n",
      "Epoch: 77, Step: 00200/483, Loss: 0.8927\n",
      "Epoch: 77, Step: 00300/483, Loss: 0.8928\n",
      "Epoch: 77, Step: 00400/483, Loss: 0.8927\n",
      "0.8926738131046296\n",
      "--- 2499.1291513442993 seconds ---\n",
      "Epoch: 78, Step: 00100/483, Loss: 0.8928\n",
      "Epoch: 78, Step: 00200/483, Loss: 0.8930\n",
      "Epoch: 78, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 78, Step: 00400/483, Loss: 0.8929\n",
      "0.8928688198328019\n",
      "--- 2531.495712995529 seconds ---\n",
      "Epoch: 79, Step: 00100/483, Loss: 0.8930\n",
      "Epoch: 79, Step: 00200/483, Loss: 0.8927\n",
      "Epoch: 79, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 79, Step: 00400/483, Loss: 0.8929\n",
      "0.8928617173433304\n",
      "--- 2563.9731986522675 seconds ---\n",
      "Epoch: 80, Step: 00100/483, Loss: 0.8930\n",
      "Epoch: 80, Step: 00200/483, Loss: 0.8927\n",
      "Epoch: 80, Step: 00300/483, Loss: 0.8925\n",
      "Epoch: 80, Step: 00400/483, Loss: 0.8921\n",
      "0.8921366184949875\n",
      "--- 2596.3040025234222 seconds ---\n",
      "Epoch: 81, Step: 00100/483, Loss: 0.8932\n",
      "Epoch: 81, Step: 00200/483, Loss: 0.8927\n",
      "Epoch: 81, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 81, Step: 00400/483, Loss: 0.8926\n",
      "0.8925974845886231\n",
      "--- 2628.7473154067993 seconds ---\n",
      "Epoch: 82, Step: 00100/483, Loss: 0.8925\n",
      "Epoch: 82, Step: 00200/483, Loss: 0.8926\n",
      "Epoch: 82, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 82, Step: 00400/483, Loss: 0.8925\n",
      "0.892548395395279\n",
      "--- 2661.1983671188354 seconds ---\n",
      "Epoch: 83, Step: 00100/483, Loss: 0.8929\n",
      "Epoch: 83, Step: 00200/483, Loss: 0.8927\n",
      "Epoch: 83, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 83, Step: 00400/483, Loss: 0.8928\n",
      "0.8927788823843003\n",
      "--- 2693.622525215149 seconds ---\n",
      "Epoch: 84, Step: 00100/483, Loss: 0.8927\n",
      "Epoch: 84, Step: 00200/483, Loss: 0.8928\n",
      "Epoch: 84, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 84, Step: 00400/483, Loss: 0.8923\n",
      "0.8923308295011521\n",
      "--- 2726.0874195098877 seconds ---\n",
      "Epoch: 85, Step: 00100/483, Loss: 0.8928\n",
      "Epoch: 85, Step: 00200/483, Loss: 0.8927\n",
      "Epoch: 85, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 85, Step: 00400/483, Loss: 0.8924\n",
      "0.8923731827735901\n",
      "--- 2758.612877368927 seconds ---\n",
      "Epoch: 86, Step: 00100/483, Loss: 0.8927\n",
      "Epoch: 86, Step: 00200/483, Loss: 0.8926\n",
      "Epoch: 86, Step: 00300/483, Loss: 0.8927\n",
      "Epoch: 86, Step: 00400/483, Loss: 0.8923\n",
      "0.8923497855663299\n",
      "--- 2791.0326759815216 seconds ---\n",
      "Epoch: 87, Step: 00100/483, Loss: 0.8925\n",
      "Epoch: 87, Step: 00200/483, Loss: 0.8924\n",
      "Epoch: 87, Step: 00300/483, Loss: 0.8922\n",
      "Epoch: 87, Step: 00400/483, Loss: 0.8925\n",
      "0.8925103104114532\n",
      "--- 2823.5336334705353 seconds ---\n",
      "Epoch: 88, Step: 00100/483, Loss: 0.8932\n",
      "Epoch: 88, Step: 00200/483, Loss: 0.8924\n",
      "Epoch: 88, Step: 00300/483, Loss: 0.8925\n",
      "Epoch: 88, Step: 00400/483, Loss: 0.8923\n",
      "0.8923158371448516\n",
      "--- 2855.9178380966187 seconds ---\n",
      "Epoch: 89, Step: 00100/483, Loss: 0.8927\n",
      "Epoch: 89, Step: 00200/483, Loss: 0.8920\n",
      "Epoch: 89, Step: 00300/483, Loss: 0.8925\n",
      "Epoch: 89, Step: 00400/483, Loss: 0.8921\n",
      "0.8921224987506866\n",
      "--- 2888.331136226654 seconds ---\n",
      "Epoch: 90, Step: 00100/483, Loss: 0.8928\n",
      "Epoch: 90, Step: 00200/483, Loss: 0.8923\n",
      "Epoch: 90, Step: 00300/483, Loss: 0.8923\n",
      "Epoch: 90, Step: 00400/483, Loss: 0.8922\n",
      "0.8921512722969055\n",
      "--- 2920.722516298294 seconds ---\n",
      "Epoch: 91, Step: 00100/483, Loss: 0.8930\n",
      "Epoch: 91, Step: 00200/483, Loss: 0.8923\n",
      "Epoch: 91, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 91, Step: 00400/483, Loss: 0.8925\n",
      "0.8924717283248902\n",
      "--- 2953.1165215969086 seconds ---\n",
      "Epoch: 92, Step: 00100/483, Loss: 0.8922\n",
      "Epoch: 92, Step: 00200/483, Loss: 0.8925\n",
      "Epoch: 92, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 92, Step: 00400/483, Loss: 0.8924\n",
      "0.8923542821407318\n",
      "--- 2985.507579088211 seconds ---\n",
      "Epoch: 93, Step: 00100/483, Loss: 0.8928\n",
      "Epoch: 93, Step: 00200/483, Loss: 0.8924\n",
      "Epoch: 93, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 93, Step: 00400/483, Loss: 0.8926\n",
      "0.8925516259670258\n",
      "--- 3017.8802597522736 seconds ---\n",
      "Epoch: 94, Step: 00100/483, Loss: 0.8926\n",
      "Epoch: 94, Step: 00200/483, Loss: 0.8926\n",
      "Epoch: 94, Step: 00300/483, Loss: 0.8920\n",
      "Epoch: 94, Step: 00400/483, Loss: 0.8922\n",
      "0.8922447562217712\n",
      "--- 3050.3343391418457 seconds ---\n",
      "Epoch: 95, Step: 00100/483, Loss: 0.8924\n",
      "Epoch: 95, Step: 00200/483, Loss: 0.8925\n",
      "Epoch: 95, Step: 00300/483, Loss: 0.8926\n",
      "Epoch: 95, Step: 00400/483, Loss: 0.8924\n",
      "0.8923623412847519\n",
      "--- 3082.8398444652557 seconds ---\n",
      "Epoch: 96, Step: 00100/483, Loss: 0.8928\n",
      "Epoch: 96, Step: 00200/483, Loss: 0.8926\n",
      "Epoch: 96, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 96, Step: 00400/483, Loss: 0.8922\n",
      "0.892175430059433\n",
      "--- 3115.3244988918304 seconds ---\n",
      "Epoch: 97, Step: 00100/483, Loss: 0.8926\n",
      "Epoch: 97, Step: 00200/483, Loss: 0.8926\n",
      "Epoch: 97, Step: 00300/483, Loss: 0.8917\n",
      "Epoch: 97, Step: 00400/483, Loss: 0.8924\n",
      "0.8924096208810807\n",
      "--- 3147.740090370178 seconds ---\n",
      "Epoch: 98, Step: 00100/483, Loss: 0.8924\n",
      "Epoch: 98, Step: 00200/483, Loss: 0.8922\n",
      "Epoch: 98, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 98, Step: 00400/483, Loss: 0.8924\n",
      "0.8924215185642242\n",
      "--- 3180.187920331955 seconds ---\n",
      "Epoch: 99, Step: 00100/483, Loss: 0.8928\n",
      "Epoch: 99, Step: 00200/483, Loss: 0.8924\n",
      "Epoch: 99, Step: 00300/483, Loss: 0.8924\n",
      "Epoch: 99, Step: 00400/483, Loss: 0.8921\n",
      "0.8921031039953232\n",
      "--- 3212.5814776420593 seconds ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/2816664050.py:268: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  normal_path = list(torch.load(dir_save + dir_normal_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10078, 16250,   116], device='cuda:0')\n",
      "433\n",
      "31283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19/2816664050.py:275: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  all_malicious_path = list(torch.load(dir_save + dir_all_malicious_path))\n",
      "/tmp/ipykernel_19/2816664050.py:282: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  train_val_malicious_path = list(torch.load(dir_save + dir_train_val_malicious_path))\n",
      "/tmp/ipykernel_19/2816664050.py:288: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  test_malicious_path = list(torch.load(dir_save + dir_test_malicious_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "# Reaches around 91.8% Micro-F1 after 5 epochs.\n",
    "\n",
    "from asyncio import FastChildWatcher\n",
    "from curses import meta\n",
    "from locale import normalize\n",
    "import os.path as osp\n",
    "from typing import final\n",
    "from xxlimited import new\n",
    "import os, datetime\n",
    "import torch\n",
    "import time\n",
    "import os, datetime\n",
    "from torch_geometric.datasets import AMiner\n",
    "from tqdm import tqdm\n",
    "# import torch.multiprocessing\n",
    "# torch.multiprocessing.set_sharing_strategy('file_system')\n",
    "import math\n",
    "\n",
    "debug_mode = False\n",
    "epoch = 100\n",
    "dir_save = '/kaggle/input/graph-ben-ngoai/graph_data_20251120161219'\n",
    "data = torch.load(dir_save + '/graph_data_torch.pt')\n",
    "print(data)\n",
    "\n",
    "metapath_strat = \"CUC\"\n",
    "\n",
    "# C -> U -> C -> U -> C\n",
    "if metapath_strat == \"CUC\":\n",
    "    metapath = [\n",
    "        ('Computer', 'Logon_rev', 'User'),\n",
    "        ('User', 'Logon', 'Computer'),\n",
    "        ('Computer', 'Logon_rev', 'User'),\n",
    "        ('User', 'Logon', 'Computer')\n",
    "    ]\n",
    "elif metapath_strat == \"UCAC\":\n",
    "# U -> C -> A -> C -> A -> C -> U\n",
    "    metapath = [\n",
    "        ('User', 'Logon', 'Computer'),\n",
    "        ('Computer', 'Use', 'Auth_Type'),\n",
    "        ('Auth_Type', 'Use_rev','Computer'),\n",
    "        ('Computer', 'Use', 'Auth_Type'),\n",
    "        ('Auth_Type', 'Use_rev','Computer'),\n",
    "        ('Computer', 'Logon_rev', 'User')\n",
    "    ]\n",
    "elif metapath_strat == \"UCCA\":\n",
    "    metapath = [\n",
    "        ('User', 'Logon', 'Computer'),\n",
    "        ('Computer', 'Connect', 'Computer'),\n",
    "        ('Computer', 'Use','Auth_Type'),\n",
    "        ('Auth_Type', 'Use_rev', 'Computer'),\n",
    "        ('Computer', 'Connect','Computer'),\n",
    "        ('Computer', 'Logon_rev', 'User')\n",
    "    ]\n",
    "elif metapath_strat == \"UCC\":\n",
    "    metapath = [\n",
    "        ('User', 'Logon', 'Computer'),\n",
    "        ('Computer', 'Connect', 'Computer'),\n",
    "        ('Computer', 'Connect', 'Computer'),\n",
    "        ('Computer', 'Logon_rev', 'User')\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "model = MetaPath2Vec(data.edge_index_dict, embedding_dim=128,\n",
    "                    metapath=metapath, walk_length=100, context_size=10,\n",
    "                    walks_per_node=8, num_negative_samples=5,\n",
    "                    sparse=True).to(device)\n",
    "\n",
    "loader = model.loader(batch_size=32, shuffle=True, num_workers=6)\n",
    "optimizer = torch.optim.SparseAdam(list(model.parameters()), lr=0.005)\n",
    "\n",
    "\n",
    "\n",
    "# generate metapath\n",
    "\n",
    "\n",
    "\n",
    "def generate_metapath_sampling(sample=20000):\n",
    "    node_num = len(metapath) + 1\n",
    "    model_sample = MetaPath2Vec(data.edge_index_dict, embedding_dim=128,\n",
    "                        metapath=metapath, walk_length=node_num, context_size=node_num,\n",
    "                        walks_per_node=1, num_negative_samples=1,\n",
    "                        sparse=True).to(device)\n",
    "    loader_sample = model_sample.loader(batch_size=1, shuffle=False, num_workers=1)\n",
    "    optimizer = torch.optim.SparseAdam(list(model_sample.parameters()), lr=0.001)\n",
    "    print(model_sample.start)\n",
    "    print(model_sample.end)\n",
    "    model_sample.train()\n",
    "    temp = []\n",
    "    for i, (pos_rw, neg_rw) in enumerate(tqdm(loader_sample)):\n",
    "        temp.append(pos_rw.to(device)[0])\n",
    "        if i > sample:\n",
    "            break\n",
    "    path = temp\n",
    "\n",
    "    new_path = []\n",
    "    print(path[1])\n",
    "    for i in range(len(path)):\n",
    "        if int(list(path[i])[-1]) != int(list(path[i])[-2]):\n",
    "            new_path.append(path[i])\n",
    "    # print(len(new_path))\n",
    "    path = new_path\n",
    "\n",
    "\n",
    "    temp = []\n",
    "    for i in range(len(path)):\n",
    "\n",
    "\n",
    "        if len(metapath)%2 == 0:\n",
    "            temp.append(path[i][:math.ceil((len(metapath)+1)/2)])\n",
    "            temp.append(path[i][math.ceil((len(metapath)+1)/2)-1:])\n",
    "        else:\n",
    "            temp.append(path[i][:math.ceil((len(metapath)+1)/2)])\n",
    "            temp.append(path[i][math.ceil((len(metapath)+1)/2):])\n",
    "    path = temp\n",
    "    path = list(set(path))\n",
    "    # print(path)\n",
    "    print(len(path))\n",
    "    return temp, model_sample.start, model_sample.end\n",
    "\n",
    "\n",
    "\n",
    "# remember re-implement this function when \n",
    "\n",
    "def reindexing_path(model, malicious_path, path_type):\n",
    "# ''' malicious_path of each node type are arranging from 0\n",
    "# however, in metapath2vec implementation, they are convert to \n",
    "# a unique indexing (i.e, computer from 0 -> 32131, user from 32131 -> 42213). \n",
    "# This function will convert to metapath2vec reference indexing. '''\n",
    "    new_path = []\n",
    "    if path_type == \"CUC\":\n",
    "        for i in malicious_path:\n",
    "            first = i[0] + model.start[\"Computer\"]\n",
    "            sec = i[1] + model.start[\"User\"]\n",
    "            third = i[2] + model.start[\"Computer\"]\n",
    "            new_path.append((first, sec, third))\n",
    "        # print(malicious_path)\n",
    "\n",
    "    elif path_type == \"UCAC\":\n",
    "        for i in malicious_path:\n",
    "            first = i[0] + model.start[\"User\"]\n",
    "            sec = i[1] + model.start[\"Computer\"]\n",
    "            third = i[2] + model.start[\"Auth_Type\"]\n",
    "            forth = i[3] + model.start[\"Computer\"]\n",
    "            new_path.append((first, sec, third, forth))\n",
    "    elif path_type == \"UCCA\":\n",
    "        for i in malicious_path:\n",
    "            first = i[0] + model.start[\"User\"]\n",
    "            sec = i[1] + model.start[\"Computer\"]\n",
    "            third = i[2] + model.start[\"Computer\"]\n",
    "            forth = i[3] + model.start[\"Auth_Type\"]\n",
    "            new_path.append((first, sec, third, forth))\n",
    "\n",
    "    elif path_type == \"UCC\":\n",
    "        for i in malicious_path:\n",
    "            first = i[0] + model.start[\"User\"]\n",
    "            sec = i[1] + model.start[\"Computer\"]\n",
    "            third = i[2] + model.start[\"Computer\"]\n",
    "            new_path.append((first, sec, third))\n",
    "    return new_path\n",
    "\n",
    "\n",
    "\n",
    "# if auth = \n",
    "\n",
    "\n",
    "path, start_idx, end_idx = generate_metapath_sampling(100000) \n",
    "\n",
    "# construct C->U->C path\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train(epoch, log_steps=100, eval_steps=2000):\n",
    "    model.train()\n",
    "    # print(model._pos_sample())\n",
    "    total_loss = 0\n",
    "    final_loss = 0\n",
    "    for i, (pos_rw, neg_rw) in enumerate(loader):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = model.loss(pos_rw.to(device), neg_rw.to(device))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        if (i + 1) % log_steps == 0:\n",
    "            print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "                        f'Loss: {total_loss / log_steps:.4f}'))\n",
    "            final_loss = total_loss/log_steps\n",
    "\n",
    "            total_loss = 0\n",
    "    print(final_loss)\n",
    "    return final_loss\n",
    "        # if (i + 1) % eval_steps == 0:\n",
    "        #     acc = test()\n",
    "        #     print((f'Epoch: {epoch}, Step: {i + 1:05d}/{len(loader)}, '\n",
    "        #            f'Acc: {acc:.4f}'))\n",
    "\n",
    "\n",
    "if debug_mode == True:\n",
    "    train(1)\n",
    "else:\n",
    "    start_time = time.time()\n",
    "    lowest_loss = 10000\n",
    "    best_model = None\n",
    "    for epoch in range(1, epoch):\n",
    "        loss = train(epoch)\n",
    "        if loss < lowest_loss:\n",
    "            best_model = model\n",
    "            lowest_loss = loss\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    # acc = test()\n",
    "    # print(f'Epoch: {epoch}, Accuracy: {acc:.4f}')\n",
    "\n",
    "# method 1: get embedding from the skip-gram\n",
    "# out = []\n",
    "# for i in range(len(path)):\n",
    "#     out.append(model.get_embedding(path[i]))\n",
    "# print(out[1].size())\n",
    "\n",
    "\n",
    "\n",
    "# mix_graph = True # mix graph = True, the malicious path will be sample from the whole graph \n",
    "# mix_graph = False, path will only be sampled from the red team file.\n",
    "\n",
    "# if mix_graph == False:\n",
    "\n",
    "dir_normal_path = \"/normal_path_\" + metapath_strat + \".pt\"\n",
    "normal_path = list(torch.load(dir_save + dir_normal_path))\n",
    "normal_path = reindexing_path(model, normal_path, metapath_strat)\n",
    "normal_path_tensor = [torch.LongTensor(i).to(device) for i in normal_path]\n",
    "\n",
    "print(normal_path_tensor[1])\n",
    "    \n",
    "dir_all_malicious_path = \"/all_malicious_path_\" + metapath_strat + \".pt\"\n",
    "all_malicious_path = list(torch.load(dir_save + dir_all_malicious_path))\n",
    "all_malicious_path = reindexing_path(model, all_malicious_path, metapath_strat)\n",
    "all_malicious_path_tensor = [torch.LongTensor(i).to(device) for i in all_malicious_path]\n",
    "\n",
    "# print(all_malicious_path_tensor)\n",
    "\n",
    "dir_train_val_malicious_path = \"/train_val_malicious_path_\" + metapath_strat + \".pt\"\n",
    "train_val_malicious_path = list(torch.load(dir_save + dir_train_val_malicious_path))\n",
    "train_val_malicious_path = reindexing_path(model, train_val_malicious_path, metapath_strat)\n",
    "train_val_malicious_path_tensor = [torch.LongTensor(i).to(device) for i in train_val_malicious_path]\n",
    "\n",
    "\n",
    "dir_test_malicious_path = \"/test_malicious_path_\" + metapath_strat + \".pt\"\n",
    "test_malicious_path = list(torch.load(dir_save + dir_test_malicious_path))\n",
    "test_malicious_path = reindexing_path(model, test_malicious_path, metapath_strat)\n",
    "test_malicious_path_tensor = [torch.LongTensor(i).to(device) for i in test_malicious_path]\n",
    "\n",
    "# print(malicious_path)\n",
    "labels = [0 for i in range(len(path))] + [1 for i in range(len(all_malicious_path))]\n",
    "path = path + all_malicious_path_tensor\n",
    "print(len(all_malicious_path_tensor))\n",
    "print(len(path))\n",
    "# else:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "out = dict()\n",
    "for i in range(len(path)):\n",
    "    out[path[i]] = model.get_embedding(path[i])\n",
    "print(out[path[1]].size())\n",
    "# print(out[1])\n",
    "\n",
    "\n",
    "\n",
    "out_normal = dict()\n",
    "for i in range(len(normal_path)):\n",
    "    out_normal[normal_path[i]] = model.get_embedding(normal_path_tensor[i])\n",
    "\n",
    "\n",
    "out_mal_train_val = dict()\n",
    "for i in range(len(train_val_malicious_path)):\n",
    "    out_mal_train_val[train_val_malicious_path[i]] = model.get_embedding(train_val_malicious_path_tensor[i])\n",
    "\n",
    "\n",
    "out_mal_test = dict()\n",
    "for i in range(len(test_malicious_path)):\n",
    "    out_mal_test[test_malicious_path[i]] = model.get_embedding(test_malicious_path_tensor[i])\n",
    "\n",
    "\n",
    "\n",
    "datestring = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "cur_dir = os.getcwd()\n",
    "store_directory = cur_dir + \"/model_\" + metapath_strat + \"_\"+ datestring\n",
    "os.mkdir(store_directory)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "torch.save(best_model, store_directory + '/model.pt')\n",
    "torch.save(path, store_directory + '/path.pt')\n",
    "torch.save(out, store_directory + '/path_embedding.pt')\n",
    "# torch.save(out2, store_directory + '/path_embedding_2.pt')\n",
    "torch.save(labels, store_directory + '/path_labels.pt')\n",
    "\n",
    "torch.save(out_normal, store_directory + '/out_normal.pt')\n",
    "torch.save(out_mal_train_val, store_directory + '/out_mal_train_val.pt')\n",
    "torch.save(out_mal_test, store_directory + '/out_mal_test.pt')\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "sourceId": 280004749,
     "sourceType": "kernelVersion"
    },
    {
     "sourceId": 280328133,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31193,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3641.195995,
   "end_time": "2025-11-20T17:56:46.484034",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-20T16:56:05.288039",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
