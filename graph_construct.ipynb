{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584099d5",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-11-20T14:52:59.596892Z",
     "iopub.status.busy": "2025-11-20T14:52:59.596696Z",
     "iopub.status.idle": "2025-11-20T14:53:00.973568Z",
     "shell.execute_reply": "2025-11-20T14:53:00.972736Z"
    },
    "papermill": {
     "duration": 1.381087,
     "end_time": "2025-11-20T14:53:00.974871",
     "exception": false,
     "start_time": "2025-11-20T14:52:59.593784",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/lmtrackerdata/auth.txt\n",
      "/kaggle/input/lmtrackerdata/dns.txt\n",
      "/kaggle/input/lmtrackerdata/redteam.txt\n",
      "/kaggle/input/lmtrackerdata/flows.txt\n",
      "/kaggle/input/lmtrackerdata/proc.txt\n",
      "/kaggle/input/extract-redteam-auth/red_team_auth.txt\n",
      "/kaggle/input/extract-redteam-auth/__results__.html\n",
      "/kaggle/input/extract-redteam-auth/__notebook__.ipynb\n",
      "/kaggle/input/extract-redteam-auth/__output__.json\n",
      "/kaggle/input/extract-redteam-auth/red_team_list.pkl\n",
      "/kaggle/input/extract-redteam-auth/custom.css\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21d78b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:53:00.980195Z",
     "iopub.status.busy": "2025-11-20T14:53:00.979881Z",
     "iopub.status.idle": "2025-11-20T14:55:27.084620Z",
     "shell.execute_reply": "2025-11-20T14:55:27.083863Z"
    },
    "papermill": {
     "duration": 146.108704,
     "end_time": "2025-11-20T14:55:27.085911",
     "exception": false,
     "start_time": "2025-11-20T14:53:00.977207",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torchdata 0.11.0\r\n",
      "Uninstalling torchdata-0.11.0:\r\n",
      "  Successfully uninstalled torchdata-0.11.0\r\n",
      "\u001b[33mWARNING: Skipping dgl as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\u001b[33mWARNING: Skipping torch_geometric as it is not installed.\u001b[0m\u001b[33m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m47.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m69.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m45.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m14.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\r\n",
      "torchtune 0.6.1 requires torchdata==0.11.0, but you have torchdata 0.7.0 which is incompatible.\r\n",
      "libcugraph-cu12 25.6.0 requires libraft-cu12==25.6.*, but you have libraft-cu12 25.2.0 which is incompatible.\r\n",
      "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\r\n",
      "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.1.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires pylibraft-cu12==25.6.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\r\n",
      "pylibcugraph-cu12 25.6.0 requires rmm-cu12==25.6.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m52.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.7/63.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y torchdata dgl torch_geometric\n",
    "\n",
    "!pip install -q torchdata==0.7.0\n",
    "\n",
    "!pip install -q dgl -f https://data.dgl.ai/wheels/repo.html\n",
    "\n",
    "!pip install -q torch_geometric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f43e7c89",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:55:27.149944Z",
     "iopub.status.busy": "2025-11-20T14:55:27.149685Z",
     "iopub.status.idle": "2025-11-20T14:55:37.837424Z",
     "shell.execute_reply": "2025-11-20T14:55:37.836568Z"
    },
    "papermill": {
     "duration": 10.72113,
     "end_time": "2025-11-20T14:55:37.838633",
     "exception": false,
     "start_time": "2025-11-20T14:55:27.117503",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DGL backend not selected or invalid.  Assuming PyTorch for now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the default backend to \"pytorch\". You can change it in the ~/.dgl/config.json file or export the DGLBACKEND environment variable.  Valid options are: pytorch, mxnet, tensorflow (all lowercase)\n",
      "torchdata: 0.7.0\n",
      "dgl: 2.1.0\n",
      "PyG: 2.7.0\n"
     ]
    }
   ],
   "source": [
    "import torchdata\n",
    "from torchdata.datapipes.iter import IterableWrapper\n",
    "import dgl\n",
    "import torch_geometric\n",
    "\n",
    "print(\"torchdata:\", torchdata.__version__)\n",
    "print(\"dgl:\", dgl.__version__)\n",
    "print(\"PyG:\", torch_geometric.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435f64e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:55:37.902507Z",
     "iopub.status.busy": "2025-11-20T14:55:37.902010Z",
     "iopub.status.idle": "2025-11-20T14:55:37.909068Z",
     "shell.execute_reply": "2025-11-20T14:55:37.908480Z"
    },
    "papermill": {
     "duration": 0.03974,
     "end_time": "2025-11-20T14:55:37.910081",
     "exception": false,
     "start_time": "2025-11-20T14:55:37.870341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sử dụng thiết bị: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from csv import reader\n",
    "from torch_geometric.data import HeteroData\n",
    "import time\n",
    "import os, datetime\n",
    "import random\n",
    "\n",
    "# --- THIẾT LẬP KAGLLE VÀ CUDA ---\n",
    "KAGGLE_DATA_PATH = \"/kaggle/input/lanl-data/\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Sử dụng thiết bị: {device}\")\n",
    "\n",
    "# Thiết lập Seed để đảm bảo tính lặp lại (Reproducibility)\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(SEED)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc5ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-20T14:55:37.973659Z",
     "iopub.status.busy": "2025-11-20T14:55:37.973474Z",
     "iopub.status.idle": "2025-11-20T16:12:24.490582Z",
     "shell.execute_reply": "2025-11-20T16:12:24.489675Z"
    },
    "papermill": {
     "duration": 4606.550582,
     "end_time": "2025-11-20T16:12:24.491951",
     "exception": false,
     "start_time": "2025-11-20T14:55:37.941369",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Import auth.txt File --------------\n",
      "--- 33.39685344696045 seconds ---\n",
      "10.0Mth lines\n",
      "--- 71.81239771842957 seconds ---\n",
      "20.0Mth lines\n",
      "--- 125.29714035987854 seconds ---\n",
      "30.0Mth lines\n",
      "--- 173.24480652809143 seconds ---\n",
      "40.0Mth lines\n",
      "--- 220.64695811271667 seconds ---\n",
      "50.0Mth lines\n",
      "--- 254.967449426651 seconds ---\n",
      "60.0Mth lines\n",
      "--- 288.86380434036255 seconds ---\n",
      "70.0Mth lines\n",
      "--- 328.8301274776459 seconds ---\n",
      "80.0Mth lines\n",
      "--- 384.68598675727844 seconds ---\n",
      "90.0Mth lines\n",
      "--- 435.06723833084106 seconds ---\n",
      "100.0Mth lines\n",
      "--- 493.16895818710327 seconds ---\n",
      "110.0Mth lines\n",
      "--- 545.2628283500671 seconds ---\n",
      "120.0Mth lines\n",
      "--- 603.2521634101868 seconds ---\n",
      "130.0Mth lines\n",
      "--- 656.6910541057587 seconds ---\n",
      "140.0Mth lines\n",
      "--- 713.875804901123 seconds ---\n",
      "150.0Mth lines\n",
      "--- 764.7284688949585 seconds ---\n",
      "160.0Mth lines\n",
      "--- 816.115241765976 seconds ---\n",
      "170.0Mth lines\n",
      "--- 850.255270242691 seconds ---\n",
      "180.0Mth lines\n",
      "--- 884.4800601005554 seconds ---\n",
      "190.0Mth lines\n",
      "--- 919.7329676151276 seconds ---\n",
      "200.0Mth lines\n",
      "--- 974.9437940120697 seconds ---\n",
      "210.0Mth lines\n",
      "--- 1029.848070859909 seconds ---\n",
      "220.0Mth lines\n",
      "--- 1086.3366346359253 seconds ---\n",
      "230.0Mth lines\n",
      "--- 1140.955305814743 seconds ---\n",
      "240.0Mth lines\n",
      "--- 1195.2918977737427 seconds ---\n",
      "250.0Mth lines\n",
      "--- 1250.16450881958 seconds ---\n",
      "260.0Mth lines\n",
      "--- 1308.015591621399 seconds ---\n",
      "270.0Mth lines\n",
      "--- 1365.0674707889557 seconds ---\n",
      "280.0Mth lines\n",
      "--- 1402.8801367282867 seconds ---\n",
      "290.0Mth lines\n",
      "--- 1442.2310087680817 seconds ---\n",
      "300.0Mth lines\n",
      "--- 1481.6052720546722 seconds ---\n",
      "310.0Mth lines\n",
      "--- 1522.025891304016 seconds ---\n",
      "320.0Mth lines\n",
      "--- 1562.1615409851074 seconds ---\n",
      "330.0Mth lines\n",
      "--- 1612.1007406711578 seconds ---\n",
      "340.0Mth lines\n",
      "--- 1672.5101253986359 seconds ---\n",
      "350.0Mth lines\n",
      "--- 1730.2860345840454 seconds ---\n",
      "360.0Mth lines\n",
      "--- 1789.8102631568909 seconds ---\n",
      "370.0Mth lines\n",
      "--- 1849.5710253715515 seconds ---\n",
      "380.0Mth lines\n",
      "--- 1906.9913280010223 seconds ---\n",
      "390.0Mth lines\n",
      "--- 1941.7701387405396 seconds ---\n",
      "400.0Mth lines\n",
      "--- 1975.0571682453156 seconds ---\n",
      "410.0Mth lines\n",
      "--- 2009.18882894516 seconds ---\n",
      "420.0Mth lines\n",
      "--- 2044.4208788871765 seconds ---\n",
      "430.0Mth lines\n",
      "--- 2097.686413526535 seconds ---\n",
      "440.0Mth lines\n",
      "--- 2159.880452632904 seconds ---\n",
      "450.0Mth lines\n",
      "--- 2220.1541044712067 seconds ---\n",
      "460.0Mth lines\n",
      "--- 2282.4000992774963 seconds ---\n",
      "470.0Mth lines\n",
      "--- 2344.3457329273224 seconds ---\n",
      "480.0Mth lines\n",
      "--- 2405.667758703232 seconds ---\n",
      "490.0Mth lines\n",
      "--- 2467.430052757263 seconds ---\n",
      "500.0Mth lines\n",
      "--- 2525.799687385559 seconds ---\n",
      "510.0Mth lines\n",
      "--- 2563.661238670349 seconds ---\n",
      "520.0Mth lines\n",
      "--- 2599.5780096054077 seconds ---\n",
      "530.0Mth lines\n",
      "--- 2635.6733367443085 seconds ---\n",
      "540.0Mth lines\n",
      "--- 2672.4830536842346 seconds ---\n",
      "550.0Mth lines\n",
      "--- 2710.5133283138275 seconds ---\n",
      "560.0Mth lines\n",
      "--- 2748.4013390541077 seconds ---\n",
      "570.0Mth lines\n",
      "--- 2788.5735142230988 seconds ---\n",
      "580.0Mth lines\n",
      "--- 2827.367842912674 seconds ---\n",
      "590.0Mth lines\n",
      "--- 2866.4008429050446 seconds ---\n",
      "600.0Mth lines\n",
      "--- 2904.3786885738373 seconds ---\n",
      "610.0Mth lines\n",
      "--- 2943.1046464443207 seconds ---\n",
      "620.0Mth lines\n",
      "--- 2982.577802181244 seconds ---\n",
      "630.0Mth lines\n",
      "--- 3023.7444717884064 seconds ---\n",
      "640.0Mth lines\n",
      "--- 3064.3568873405457 seconds ---\n",
      "650.0Mth lines\n",
      "--- 3105.663863182068 seconds ---\n",
      "660.0Mth lines\n",
      "--- 3146.0813086032867 seconds ---\n",
      "670.0Mth lines\n",
      "--- 3185.887920856476 seconds ---\n",
      "680.0Mth lines\n",
      "--- 3226.5660405158997 seconds ---\n",
      "690.0Mth lines\n",
      "--- 3263.3293821811676 seconds ---\n",
      "700.0Mth lines\n",
      "--- 3302.8174617290497 seconds ---\n",
      "710.0Mth lines\n",
      "--- 3340.5946910381317 seconds ---\n",
      "720.0Mth lines\n",
      "--- 3381.0220108032227 seconds ---\n",
      "730.0Mth lines\n",
      "--- 3420.9244544506073 seconds ---\n",
      "740.0Mth lines\n",
      "--- 3464.5442883968353 seconds ---\n",
      "750.0Mth lines\n",
      "--- 3505.9212448596954 seconds ---\n",
      "760.0Mth lines\n",
      "--- 3542.4359974861145 seconds ---\n",
      "770.0Mth lines\n",
      "--- 3582.155383825302 seconds ---\n",
      "780.0Mth lines\n",
      "--- 3620.1818804740906 seconds ---\n",
      "790.0Mth lines\n",
      "--- 3659.290737390518 seconds ---\n",
      "800.0Mth lines\n",
      "--- 3697.365133523941 seconds ---\n",
      "810.0Mth lines\n",
      "--- 3734.7186748981476 seconds ---\n",
      "820.0Mth lines\n",
      "--- 3772.3812172412872 seconds ---\n",
      "830.0Mth lines\n",
      "--- 3812.109696626663 seconds ---\n",
      "840.0Mth lines\n",
      "--- 3848.453667640686 seconds ---\n",
      "850.0Mth lines\n",
      "--- 3884.6976613998413 seconds ---\n",
      "860.0Mth lines\n",
      "--- 3920.585245370865 seconds ---\n",
      "870.0Mth lines\n",
      "--- 3964.5475239753723 seconds ---\n",
      "880.0Mth lines\n",
      "--- 4008.1776716709137 seconds ---\n",
      "890.0Mth lines\n",
      "--- 4053.3484404087067 seconds ---\n",
      "900.0Mth lines\n",
      "--- 4097.291304826736 seconds ---\n",
      "910.0Mth lines\n",
      "--- 4141.002523183823 seconds ---\n",
      "920.0Mth lines\n",
      "--- 4178.115414142609 seconds ---\n",
      "930.0Mth lines\n",
      "--- 4215.519737958908 seconds ---\n",
      "940.0Mth lines\n",
      "--- 4251.167964458466 seconds ---\n",
      "950.0Mth lines\n",
      "--- 4286.791800260544 seconds ---\n",
      "960.0Mth lines\n",
      "--- 4322.395695447922 seconds ---\n",
      "970.0Mth lines\n",
      "--- 4356.516040802002 seconds ---\n",
      "980.0Mth lines\n",
      "--- 4391.51581120491 seconds ---\n",
      "990.0Mth lines\n",
      "--- 4426.925298213959 seconds ---\n",
      "1000.0Mth lines\n",
      "--- 4460.610075712204 seconds ---\n",
      "1010.0Mth lines\n",
      "--- 4493.437949895859 seconds ---\n",
      "1020.0Mth lines\n",
      "--- 4526.270645141602 seconds ---\n",
      "1030.0Mth lines\n",
      "--- 4559.179920434952 seconds ---\n",
      "1040.0Mth lines\n",
      "--- 4592.500560760498 seconds ---\n",
      "1050.0Mth lines\n",
      "---------Import Red Team File --------------\n",
      "------------Constructing Graph---------------\n",
      "Graph(num_nodes={'Auth_Type': 6, 'Computer': 15425, 'Logon_orient': 7, 'Logon_type': 10, 'User': 11588},\n",
      "      num_edges={('Computer', 'Connect', 'Computer'): 429628, ('Computer', 'Have', 'Logon_orient'): 61530, ('Computer', 'Use', 'Auth_Type'): 44531, ('Computer', 'Use_logon', 'Logon_type'): 64316, ('User', 'Logon', 'Computer'): 272027, ('User', 'SwitchUser', 'User'): 5634},\n",
      "      metagraph=[('Computer', 'Computer', 'Connect'), ('Computer', 'Logon_orient', 'Have'), ('Computer', 'Auth_Type', 'Use'), ('Computer', 'Logon_type', 'Use_logon'), ('User', 'Computer', 'Logon'), ('User', 'User', 'SwitchUser')])\n",
      "--- 4.384100437164307 seconds ---\n",
      "------------Storing Graph---------------\n",
      "Graph store at: /kaggle/working/graph_data_20251120161219\n",
      "--- 4.822098016738892 seconds ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from csv import reader\n",
    "from torch_geometric.data import HeteroData\n",
    "import time\n",
    "import os, datetime\n",
    "import pickle\n",
    "\n",
    "computer2nodeid = dict()\n",
    "user2nodeid = dict()\n",
    "process2nodeid = dict()\n",
    "auth_type2nodeid = dict()\n",
    "logon_type2nodeid = dict()\n",
    "logon_orient2nodeid = dict()\n",
    "ceiling_day = 30\n",
    "\n",
    "def auth_filter(row, day_filter):\n",
    "# day_filter: list of day that used to construct graph. Return True when current row in this list\n",
    "\n",
    "    day_in_second = 60*60*24\n",
    "    # if np.ceil(int(row[0]) / day_in_second) not in day_filter or row[8] == \"Fail\" or row[7] == \"LogOff\" or row[1][0] == \"C\" or row[2][0] == \"C\":\n",
    "    if np.ceil(int(row[0]) / day_in_second) not in day_filter or row[8] == \"Fail\" or row[1][0] == \"C\" or row[2][0] == \"C\":\n",
    "        return True\n",
    "    return False\n",
    "def process_filter(row, day_filter):\n",
    "# day_filter: list of day that used to construct graph. Return True when current row in this list\n",
    "\n",
    "    day_in_second = 60*60*24\n",
    "    if np.ceil(int(row[0]) / day_in_second) not in day_filter or row[4] == \"End\":\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "\n",
    "# open red_red_team.pkl \n",
    "with open(\"/kaggle/input/extract-redteam-auth/red_team_list.pkl\", \"rb\") as fp:   # Unpickling\n",
    "    red_team_line = pickle.load(fp)\n",
    "red_team_line = set(red_team_line)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"---------Import auth.txt File --------------\")\n",
    "start_time = time.time()\n",
    "# open file in read mode\n",
    "count = 0\n",
    "day_in_second = 60*60*24\n",
    "day_filter = [2, 3, 6, 7, 8, 9, 10, 13, 14, 15, 16, 21, 22, 23, 27, 28, 29, 30] # only get data from this list of day\n",
    "train_val_day = [2, 3, 6, 7, 8, 9, 10]\n",
    "test_day = [13, 14, 15, 16, 21, 22, 23, 27, 28, 29, 30]\n",
    "user_comp_dict = set()\n",
    "comp_comp_dict = set()\n",
    "user_user_dict = set()\n",
    "comp_auth_type_dict  = set()\n",
    "comp_logon_type_dict  = set()\n",
    "comp_logon_orient_dict  = set()\n",
    "\n",
    "normal_path_CUC = set()\n",
    "normal_path_UCC = set()\n",
    "normal_path_UCAC = set()\n",
    "normal_path_UCCA = set()\n",
    "\n",
    "with open('/kaggle/input/lmtrackerdata/auth.txt', 'r') as read_obj:\n",
    "    # pass the file object to reader() to get the reader object\n",
    "    csv_reader = reader(read_obj)\n",
    "    # Iterate over each row in the csv using reader object\n",
    "    for row in csv_reader:\n",
    "\n",
    "        count = count + 1\n",
    "        # skip redteam sample\n",
    "        if count in red_team_line:\n",
    "            continue\n",
    "\n",
    "\n",
    "\n",
    "        if count % 10000000 == 0:\n",
    "            print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "            print(str(count/1000000) + \"Mth lines\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        if auth_filter(row, day_filter):\n",
    "            continue\n",
    "        if np.ceil(int(row[0]) / day_in_second) > ceiling_day:\n",
    "            break\n",
    "\n",
    "\n",
    "        source_user     = row[1].split(\"@\")[0]\n",
    "        des_user        = row[2].split(\"@\")[0]\n",
    "        source_computer = row[3]\n",
    "        des_computer    = row[4]\n",
    "        auth_type       = row[5]\n",
    "        logon_type      = row[6] # \n",
    "        logon_orient    = row[7] # logon, logoff\n",
    "        \n",
    "        if \"MICROSOFT\" in auth_type:\n",
    "            auth_type = \"MICROSOFT_AUTHENTICATION_PACKAGE_V1_0\"\n",
    "            \n",
    "            \n",
    "# update node dictionary   \n",
    "        \n",
    "        if source_user not in user2nodeid:\n",
    "            user2nodeid[source_user] = len(user2nodeid) \n",
    "        if des_user not in user2nodeid:\n",
    "            user2nodeid[des_user] = len(user2nodeid) \n",
    "        if source_computer not in computer2nodeid:\n",
    "            computer2nodeid[source_computer] = len(computer2nodeid) \n",
    "        if des_computer not in computer2nodeid:\n",
    "            computer2nodeid[des_computer] = len(computer2nodeid)\n",
    "        if auth_type not in auth_type2nodeid:\n",
    "            auth_type2nodeid[auth_type] = len(auth_type2nodeid)\n",
    "        if logon_type not in logon_type2nodeid:\n",
    "            logon_type2nodeid[logon_type] = len(logon_type2nodeid)\n",
    "        if logon_orient not in logon_orient2nodeid:\n",
    "            logon_orient2nodeid[logon_orient] = len(logon_orient2nodeid)    \n",
    "            \n",
    "# ***********Add more edge type here**********            \n",
    "# note that we consider undirecterd graph for metapath2vec\n",
    "\n",
    "# Comp -> comp edge\n",
    "        if source_computer != des_computer:\n",
    "            comp_comp_dict.add((computer2nodeid[source_computer], computer2nodeid[des_computer]))\n",
    "            comp_comp_dict.add((computer2nodeid[des_computer], computer2nodeid[source_computer]))\n",
    "# user -> comp edge\n",
    "        user_comp_dict.add((user2nodeid[source_user], computer2nodeid[source_computer]))\n",
    "        user_comp_dict.add((user2nodeid[des_user], computer2nodeid[des_computer]))\n",
    "# user -> user edge\n",
    "        if source_user != des_user:\n",
    "            user_user_dict.add((user2nodeid[source_user], user2nodeid[des_user]))\n",
    "        else:\n",
    "            # sample CUC benign path from log file\n",
    "            normal_path_CUC.add((computer2nodeid[source_computer], user2nodeid[source_user], computer2nodeid[des_computer])) \n",
    "            normal_path_UCC.add((user2nodeid[source_user], computer2nodeid[source_computer], computer2nodeid[des_computer])) \n",
    "            normal_path_UCAC.add((user2nodeid[source_user], computer2nodeid[source_computer], auth_type2nodeid[auth_type], computer2nodeid[des_computer]))    \n",
    "            normal_path_UCCA.add((user2nodeid[source_user], computer2nodeid[source_computer], computer2nodeid[des_computer], auth_type2nodeid[auth_type]))\n",
    "# auth_type -> comp \n",
    "        comp_auth_type_dict.add((computer2nodeid[source_computer], auth_type2nodeid[auth_type]))\n",
    "        comp_auth_type_dict.add((computer2nodeid[des_computer], auth_type2nodeid[auth_type]))\n",
    "        \n",
    "        comp_logon_type_dict.add((computer2nodeid[source_computer], logon_type2nodeid[logon_type]))\n",
    "        comp_logon_type_dict.add((computer2nodeid[des_computer], logon_type2nodeid[logon_type]))\n",
    "        \n",
    "        comp_logon_orient_dict.add((computer2nodeid[source_computer], logon_orient2nodeid[logon_orient]))\n",
    "        comp_logon_orient_dict.add((computer2nodeid[des_computer], logon_orient2nodeid[logon_orient]))\n",
    "            \n",
    "\n",
    "\n",
    "print(\"---------Import Red Team File --------------\")\n",
    "malicious_edge_dict = set()\n",
    "\n",
    "all_malicious_path_CUC = set()\n",
    "all_malicious_path_UCC = set()\n",
    "all_malicious_path_UCAC = set()\n",
    "all_malicious_path_UCCA = set()\n",
    "\n",
    "train_val_malicious_path_CUC = set()\n",
    "train_val_malicious_path_UCC = set()\n",
    "train_val_malicious_path_UCAC = set()\n",
    "train_val_malicious_path_UCCA = set()\n",
    "\n",
    "test_malicious_path_CUC = set()\n",
    "test_malicious_path_UCC = set()\n",
    "test_malicious_path_UCAC = set()\n",
    "test_malicious_path_UCCA = set()\n",
    "\n",
    "\n",
    "\n",
    "unseen_nodes = []\n",
    "with open('/kaggle/input/extract-redteam-auth/red_team_auth.txt', 'r') as read_obj:\n",
    "    # pass the file object to reader() to get the reader object\n",
    "    csv_reader = reader(read_obj)\n",
    "    # Iterate over each row in the csv using reader object\n",
    "    for row in csv_reader:\n",
    "        source_user     = row[1].split(\"@\")[0]\n",
    "        des_user        = row[2].split(\"@\")[0]\n",
    "        source_computer = row[3]\n",
    "        des_computer    = row[4]\n",
    "        auth_type       = row[5]\n",
    "        logon_type      = row[6] # \n",
    "        logon_orient    = row[7] # logon, logoff\n",
    "        \n",
    "        \n",
    "        if \"MICROSOFT\" in auth_type:\n",
    "            auth_type = \"MICROSOFT_AUTHENTICATION_PACKAGE_V1_0\"\n",
    "            \n",
    "            \n",
    "# update node dictionary   \n",
    "        \n",
    "        if source_user not in user2nodeid:\n",
    "            user2nodeid[source_user] = len(user2nodeid) \n",
    "        if des_user not in user2nodeid:\n",
    "            user2nodeid[des_user] = len(user2nodeid) \n",
    "        if source_computer not in computer2nodeid:\n",
    "            computer2nodeid[source_computer] = len(computer2nodeid) \n",
    "        if des_computer not in computer2nodeid:\n",
    "            computer2nodeid[des_computer] = len(computer2nodeid)\n",
    "        if auth_type not in auth_type2nodeid:\n",
    "            auth_type2nodeid[auth_type] = len(auth_type2nodeid)\n",
    "        if logon_type not in logon_type2nodeid:\n",
    "            logon_type2nodeid[logon_type] = len(logon_type2nodeid)\n",
    "        if logon_orient not in logon_orient2nodeid:\n",
    "            logon_orient2nodeid[logon_orient] = len(logon_orient2nodeid)   \n",
    "            \n",
    "            \n",
    "            \n",
    "        # user_comp_dict.add((user2nodeid[source_user], computer2nodeid[source_computer]))\n",
    "        # user_comp_dict.add((user2nodeid[des_user], computer2nodeid[des_computer]))\n",
    "        \n",
    "        malicious_edge_dict.add((user2nodeid[source_user], computer2nodeid[source_computer]))\n",
    "        malicious_edge_dict.add((user2nodeid[des_user], computer2nodeid[des_computer]))\n",
    "\n",
    "        \n",
    "# user -> src comp -> auth_type -> des comp: UCAC\n",
    "# user -> src comp -> des comp: UCC\n",
    "# src comp -> user -> des comp: CUC\n",
    "\n",
    "\n",
    "        all_malicious_path_CUC.add((computer2nodeid[source_computer], user2nodeid[source_user], computer2nodeid[des_computer]))\n",
    "        all_malicious_path_UCAC.add((user2nodeid[source_user], computer2nodeid[source_computer], auth_type2nodeid[auth_type], computer2nodeid[des_computer]))\n",
    "        all_malicious_path_UCC.add((user2nodeid[source_user], computer2nodeid[source_computer], computer2nodeid[des_computer]))\n",
    "        all_malicious_path_UCCA.add((user2nodeid[source_user], computer2nodeid[source_computer], computer2nodeid[des_computer], auth_type2nodeid[auth_type]))\n",
    "        \n",
    "        if np.ceil(int(row[0]) / day_in_second) in train_val_day:\n",
    "            train_val_malicious_path_CUC.add((computer2nodeid[source_computer], user2nodeid[source_user], computer2nodeid[des_computer]))\n",
    "            train_val_malicious_path_UCAC.add((user2nodeid[source_user], computer2nodeid[source_computer], auth_type2nodeid[auth_type], computer2nodeid[des_computer]))\n",
    "            train_val_malicious_path_UCC.add((user2nodeid[source_user], computer2nodeid[source_computer], computer2nodeid[des_computer]))        \n",
    "            train_val_malicious_path_UCCA.add((user2nodeid[source_user], computer2nodeid[source_computer], computer2nodeid[des_computer], auth_type2nodeid[auth_type]))\n",
    "                                        \n",
    "                                        \n",
    "        if np.ceil(int(row[0]) / day_in_second) in test_day:\n",
    "            test_malicious_path_CUC.add((computer2nodeid[source_computer], user2nodeid[source_user], computer2nodeid[des_computer]))\n",
    "            test_malicious_path_UCAC.add((user2nodeid[source_user], computer2nodeid[source_computer], auth_type2nodeid[auth_type], computer2nodeid[des_computer]))\n",
    "            test_malicious_path_UCC.add((user2nodeid[source_user], computer2nodeid[source_computer], computer2nodeid[des_computer]))        \n",
    "            test_malicious_path_UCAC.add((user2nodeid[source_user], computer2nodeid[source_computer], computer2nodeid[des_computer], auth_type2nodeid[auth_type]))\n",
    "\n",
    "\n",
    "print(\"------------Constructing Graph---------------\")\n",
    "start_time = time.time()\n",
    "data_dict = dict()\n",
    "data_dict[('Computer', 'Connect', 'Computer')] = list(comp_comp_dict)\n",
    "data_dict[('User', 'Logon', 'Computer')] = list(user_comp_dict)\n",
    "data_dict[('User', 'SwitchUser', 'User')] = list(user_user_dict)\n",
    "# data_dict[('Computer', 'Create', 'Process')] = list(comp_proc_dict)\n",
    "data_dict[('Computer', 'Use', 'Auth_Type')] = list(comp_auth_type_dict)\n",
    "data_dict[('Computer', 'Use_logon', 'Logon_type')] = list(comp_logon_type_dict)\n",
    "data_dict[('Computer', 'Have', 'Logon_orient')] = list(comp_logon_orient_dict)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "g = dgl.heterograph(data_dict)\n",
    "print(g)\n",
    "\n",
    "# print(comp_comp_dict)\n",
    "# gasdhjkasdgkhjas\n",
    "\n",
    "data_dict[('Computer', 'Connect', 'Computer')] = torch.transpose(torch.LongTensor(data_dict[('Computer', 'Connect', 'Computer')]), 0, 1) \n",
    "\n",
    "data_dict[('User', 'Logon', 'Computer')] = torch.transpose(torch.LongTensor(data_dict[('User', 'Logon', 'Computer')]), 0, 1) \n",
    "\n",
    "data_dict[('User', 'SwitchUser', 'User')] = torch.transpose(torch.LongTensor(data_dict[('User', 'SwitchUser', 'User')]), 0, 1) \n",
    "\n",
    "# data_dict[('Computer', 'Create', 'Process')] = torch.transpose(torch.LongTensor(data_dict[('Computer', 'Create', 'Process')]), 0, 1) \n",
    "\n",
    "data_dict[('Computer', 'Use', 'Auth_Type')] = torch.transpose(torch.LongTensor(data_dict[('Computer', 'Use', 'Auth_Type')]), 0, 1) \n",
    "\n",
    "data_dict[('Computer', 'Use_logon', 'Logon_type')] = torch.transpose(torch.LongTensor(data_dict[('Computer', 'Use_logon', 'Logon_type')]), 0, 1) \n",
    "\n",
    "data_dict[('Computer', 'Have', 'Logon_orient')] = torch.transpose(torch.LongTensor(data_dict[('Computer', 'Have', 'Logon_orient')]), 0, 1) \n",
    "\n",
    "\n",
    "\n",
    "data = HeteroData()\n",
    "data['Computer', 'Connect', 'Computer'].edge_index = data_dict[('Computer', 'Connect', 'Computer')]\n",
    "data['User', 'Logon', 'Computer'].edge_index = data_dict[('User', 'Logon', 'Computer')]\n",
    "\n",
    "data['Computer', 'Logon_rev', 'User'].edge_index = torch.flip(data_dict[('User', 'Logon', 'Computer')], [0, 1]) # flip edge\n",
    "data['User', 'SwitchUser', 'User'].edge_index = data_dict[('User', 'SwitchUser', 'User')]\n",
    "\n",
    "# data['Computer', 'Create', 'Process'].edge_index = data_dict[('Computer', 'Create', 'Process')]\n",
    "# data['Process', 'Create_rev', 'Computer'].edge_index = torch.flip(data_dict[('Computer', 'Create', 'Process')], [0, 1])# flip edge\n",
    "\n",
    "data['Computer', 'Use', 'Auth_Type'].edge_index = data_dict[('Computer', 'Use', 'Auth_Type')]\n",
    "data['Auth_Type', 'Use_rev','Computer'].edge_index = torch.flip(data_dict[('Computer', 'Use', 'Auth_Type')], [0, 1])# flip edge\n",
    "\n",
    "data['Computer', 'Use_logon', 'Logon_type'].edge_index = data_dict[('Computer', 'Use_logon', 'Logon_type')]\n",
    "data['Logon_type', 'Use_logon_rev', 'Computer'].edge_index = torch.flip(data_dict[('Computer', 'Use_logon', 'Logon_type')], [0, 1])# flip edge\n",
    "\n",
    "data['Computer', 'Have', 'Logon_orient'].edge_index = data_dict[('Computer', 'Have', 'Logon_orient')]\n",
    "data['Logon_orient', 'Have_rev', 'Computer'].edge_index = torch.flip(data_dict[('Computer', 'Have', 'Logon_orient')], [0, 1])# flip edge\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "\n",
    "print(\"------------Storing Graph---------------\")\n",
    "start_time = time.time()\n",
    "\n",
    "datestring = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "cur_dir = os.getcwd()\n",
    "store_directory = cur_dir + \"/graph_data_\" + datestring\n",
    "\n",
    "\n",
    "print(\"Graph store at: \" + store_directory)\n",
    "\n",
    "os.mkdir(store_directory)\n",
    "\n",
    "dgl.save_graphs(store_directory + \"/graph_data_dgl.bin\",[g])\n",
    "torch.save(data, store_directory + '/graph_data_torch.pt')\n",
    "torch.save(normal_path_CUC, store_directory + '/normal_path_CUC.pt')\n",
    "torch.save(normal_path_UCC, store_directory + '/normal_path_UCC.pt')\n",
    "torch.save(normal_path_UCAC, store_directory + '/normal_path_UCAC.pt')\n",
    "torch.save(normal_path_UCCA, store_directory + '/normal_path_UCCA.pt')\n",
    "\n",
    "\n",
    "\n",
    "torch.save(all_malicious_path_CUC, store_directory + '/all_malicious_path_CUC.pt')\n",
    "torch.save(all_malicious_path_UCC, store_directory + '/all_malicious_path_UCC.pt')\n",
    "torch.save(all_malicious_path_UCAC, store_directory + '/all_malicious_path_UCAC.pt')\n",
    "torch.save(all_malicious_path_UCCA, store_directory + '/all_malicious_path_UCCA.pt')\n",
    "\n",
    "torch.save(train_val_malicious_path_CUC, store_directory + '/train_val_malicious_path_CUC.pt')\n",
    "torch.save(train_val_malicious_path_UCC, store_directory + '/train_val_malicious_path_UCC.pt')\n",
    "torch.save(train_val_malicious_path_UCAC, store_directory + '/train_val_malicious_path_UCAC.pt')\n",
    "torch.save(train_val_malicious_path_UCCA, store_directory + '/train_val_malicious_path_UCCA.pt')\n",
    "\n",
    "torch.save(test_malicious_path_CUC, store_directory + '/test_malicious_path_CUC.pt')\n",
    "torch.save(test_malicious_path_UCC, store_directory + '/test_malicious_path_UCC.pt')\n",
    "torch.save(test_malicious_path_UCAC, store_directory + '/test_malicious_path_UCAC.pt')\n",
    "torch.save(test_malicious_path_UCCA, store_directory + '/test_malicious_path_UCCA.pt')\n",
    "\n",
    "torch.save(computer2nodeid, store_directory + '/computer2nodeid.pt')\n",
    "torch.save(user2nodeid, store_directory + '/user2nodeid.pt')\n",
    "torch.save(process2nodeid, store_directory + '/process2nodeid.pt')\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 8781515,
     "sourceId": 13793318,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 280257579,
     "sourceType": "kernelVersion"
    }
   ],
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4771.056796,
   "end_time": "2025-11-20T16:12:27.145883",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-20T14:52:56.089087",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
